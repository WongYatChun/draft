\title{Chapter 8: Evaluating Excess Returns}

The task of estimating factor models and testing alphas for systematic strategies usually involves reusing the same historical data. This is somewhat unintuitive. One of the defining features of the past 40 years has been the increased recording of new datasets and their broad dissemination. Investment firms have budgets of tens of millions of dollars allocated to the purchase of market and alternative data, and to the bespoke collection of data (e.g., via web scrapes). And yet, the characteristics of traded assets, and specifically of companies, do not change on a minute-by-minute basis; and investment strategies with relatively long holding times - of the order of a day or longer - do not necessarily employ tick-by-tick data. If we record prices for a broad local investment universe at five-minute intervals, we collect 60 millions numeric data points per year ${ }^{1}$; including a security identifier and a time stamp, the required storage is of the order of gigabytes. History is not replaceable, and sometimes it is not deep. Not replaceable, in the sense that it is not easy to produce a simulated version of the past that provably reproduces all of its features. Not deep, in the sense that we do not live in a stationary world. The pace at which the real world outside of finance changes is breathtaking and accelerating, and markets are a timid reflection of it. Not even taking such change into account, the introduction of new technology, of new market microstructure designs, of new regulations, and the ongoing collective learning process of all market participants make the investing world of five years ago very different from today's. The fact that we have to rely on historical data poses a major challenge. We cannot design experiments. Our studies are observational and repeated. Financial practitioners do not have a shared protocol for experimental analysis. Even if we had one, it is far from obvious that it is the correct one. Well-established disciplines like Medicine and Psychology had shared experimental practices accompanied by experimental design, and yet they have undergone a reevaluation when their practitioners found that most of their results are not replicable [Ioannidis, 2005, Open Science Collaboration, 2015].

\footnotetext{
${ }^{1}$ Assuming 6.5 trading hours, 251 trading days and 3,000 stocks.
}

This poses a few challenges for us modelers. We have a very large number of signals types, which themselves depend on continuous tuning parameters, and we only have a limited history. This is similar to the situation faced by biostatisticians, who deal with tens of thousands of simultaneous tests in the form of responses from DNA microarray Dudoit et al. [2003], Huang et al. [2009]. The details are quite different. The "response variable" for a DNA microarray is usually discrete ("polytomous"), and responses are uncorrelated or weakly correlated. In quantitative finance, the response variable (be it return or Sharpe) is continuous, and signal correlation plays a decisive role.

This chapter has four sections. First, we list some basic best practices for data preparation and usage. Second, we describe some common backtesting practices and critique them. The third section is entirely devoted to describing a new backtesting protocol, which offers several advantages over the previous ones: it gives finite-sample uniform probabilistic bounds on the Sharpe Ratio of a large set of strategies. The last section applies the theory developed so far to simulated and historical data.

\section*{8.1. Backtesting Best Practices}

We review a few best practices for backtesting. They do not originate from some comprehensive theory. Unlike Athena, who was born fully formed from the mind of Zeus, it is an ever-incomplete, occasionally shallow body of knowledge that has formed by experimentation. Some references covering these practices are Arnott et al. [2019], Wang et al. [2014], LÃ³pez de Prado [2020].

\subsection*{8.1.1. Data Sourcing}

High-quality data are essential to backtesting, and the search for better data is a never-ending task for a researcher. There are several broad areas of concern. The first one is data sourcing. There are multiple vendors offering similar data. When comparing them, ask the following questions:
- Definition and interpretation. Perhaps the first and most important question, not only in data sourcing, but in quantitative investing, is what do the data mean? What is the exact definition of the data collected? What are their physical units? If the dataset is money-related, it should be unambiguous what is the reference currency (or currencies, for exchange rates). If the data is flow-related (i.e., measuring rates over time), the time unit should be defined. A surprising number of mistakes happens because of unit conversion errors.
- Provenance. Where are the data coming from? Does the vendor collect the
data themselves (e.g., via web scraping, or internet traffic)? More often, the vendor serves as an intermediary between a data originator and the client. In the former case, what is the collection criterion? Does the vendor sample data or collects the data exhaustively? Is the population sampling methodology sound? In the latter case, who is originating the data? Are they trustworthy?
- Completeness. This leads to the second question: completeness. Are there data that are obviously missing from the dataset such as, for example, intermittently missing prices? Are there data that are non-obviously missing, such as, for example, unrecorded consumer credit card transactions? Some of these questions can be answered by performing exploratory analysis on the data themselves, others need to be addressed with the vendor.
- Quality assurance. How does the provider ensure that the data it receives or collects are consistently of good quality? Does it have checks for change points in the data characteristics?
- Point-in-time vs. restated data. Does the provider offer data collected as of a certain date, without changing them at a later date, based on corrections and company updates? This is an instance of data leakage, which we will cover in more detail later.
- Transformations. Data are almost always transformed by the vendor. Examples are: imputation of missing data; winsorization and removal of outliers; end-of-period price calculations (last transaction, mid bid-ask price, weighted average). These transformations should be documented, evaluated, and if possible verified by the research analyst.
- Exploring alternatives and complements. Always ask the following commonsense questions: can we obtain better data, across the following three dimensions. First, are there providers offering larger coverage for the same dataset
(e.g., more securities at any point in time, deeper history, more frequent data)? Second, are there providers with better data? For example, if data are collected from broker-dealers, the alternative provider has an agreement with a larger number of participating contributors. Third, can we obtain complementary data? These are datasets that jointly with the original one, greatly increase its original value. For example, we may obtain transactional data that help us estimate short-term revenues of a company, in addition to data that give us a good estimate of their costs.

\subsection*{8.1.2. Research Process}

Every researcher has their own research process. This is part of their competitive advantage; it's indeed part of what they are, of thoughts and learned lessons accumulated over a lifetime of experiences and of studying. It would be futile to superimpose the author's overall research philosophy to that of the reader ${ }^{2}$, just in a few pages. However, there are a few steps that are uncontroversial, and are part of basic hygiene. Consider these akin to the precept to never leave home without wearing underwear.
- Data leakage. The first recommendation is to avoid data leakage. The definition of data leakage is the presence in the training data, the data available up to time $t$, of information contained in the target, i.e., returns in periods $t+1$ and later. The reference rule is to never use data in a backtest on a certain date that we are not able to use in production on that day. Detecting data leakage is more art than science, and it requires both a deep knowledge of the data (see above) and of the problem at hand. Below are a few examples.
- Survivorship bias. If we backtest the performance of a strategy over an extended period of time, considering only the stocks that have continuously traded during this period, i.e., the surviving stocks at the end of the backtest, we are subject to survivorship bias. Stocks are most often delisted because they experience large losses, trade at low share prices, become illiquid or do not meet some additional criteria for being listed on exchange. Removing them biases the investment sample toward outperformers with different characteristics than those of the broader investment universe at a point in time. For example, the survivors' liquidity, momentum, and size are larger than the universe. This is the simplest and most impactful instance of data leakage. The remedy to this issue is to: a) employ a sensible methodology for inclusion that is applicable at any point in time; b) specify a realistic and conservative rule in the backtest for the event of a delisting. For example, one could assume that the entire investment is written off. Note that the methodology in a) should be specified before backtests are initiated. Changing the inclusion rules based on the result of backtest is also an instance of data leakage, and it should be avoided. Criteria for inclusion are indeed not straightforward to specify. A common recommendation is to use an investment universe defined by a commercialbenchmark, like Russell 1000, Russell 3000, MSCI benchmarks, or commercial factor models investment universes. Note that benchmark components are always announced before ("announcement date") the effective addition date ("reconstitution date"), and the returns of the stocks are affected by the announcement of an inclusion or exclusion. In your backtest, you may want to capture this information, in order to assess how much of the performance of your quantitative strategies is affected by recent changes in the investment universe.
- Financial statements. Financial statement information for a given quarter or year should be included in the backtesting data on the day (or the day after) of their public release, not on the last quarter to which the data refer.
- Point-in-time data. Financial data used in the backtest on a given date should always be the most recent data available as of that date. If a 10Q (a quarterly financial report) is restated because of material error, the backtest should not reflect that. The strategy must be tested by allowing the presence of error in its input data.
- Price adjustments. Shares are regularly split (or reverse-split) into multiple shares. The price of the split share is adjusted accordingly. This occurs when the stock appreciates to the point one share becomes so expensive that it prevents investors from being able to buy it. In order to compute historical returns across long time series, prices and dividend are usually split-adjusted. This introduces a complication. A low stock price in the distant past indicates that the shares have been split several times in the future, likely because of high returns. The price becomes informative of future performance. The recommended remedy is to use adjusted prices only for return calculation. For feature generation, use as-of-date prices.
- Missingness. In certain cases (mostly unstructured data instances), data points are missing either because they were not made available as-of-date or because they contain sensitive information and were redacted. In the latter case, missingness may be suffering from look-ahead bias and is
informative of future returns.
- Avoidable mistakes. The amount of silly mistakes (in hindsight) that experienced, effective researchers make never ceases to amaze. For example, a stock characteristic available in a dataset had high Information Coefficient ${ }^{3}$ (IC). Upon further investigation, it was a stock split conversion factor (see the previous bullet point). As another example, because of an erroneous $t$ vs $t+1$ convention error, a researcher included the next-day return in a three-month momentum factor definition, also causing a false positive.
- Strategy development. There are some qualitative recommendations that, while missing a solid foundation, are hard to argue against.
- Have a theory (if you can). It is preferable to have a theory for every anomaly, and to pre-register the predictions of the theory before the backtest. For example, in their paper on quality, Asness et al. [2019] propose a theory guiding the development of the factor; so do Frazzini and Pedersen [2014] when they analyze the beta anomaly. With a theory as a guide, it is easier to choose a security characteristic among many, therefore reducing the number of strategies being tested; it is possible to interpret the result and believe in it more, and it is possible to critique and revise the characteristic, which is maybe not desirable (it would be nice if we got it right the first time) but necessary.
- Enforce reproducibility. Document all your strategies and make sure you can reproduce and rerun them at any time.
- Use the same setting in backtesting and in production as much as possible. By this we mean that we should use the same point-in-time data, but also the same optimization formulations, the same market impact model, and the same codebase.
- Calibrate the market impact model. When we perform a backtest, the market impact model has a "descriptive" role. It estimates the losses in efficiency from actual trading. It is not possible, however, to verify the realized market impact on historical data. In order to align backtest performance to live one, it is important not to take a market impact model at face value, especially one provided by a vendor or a partner. Instead, calibrate its parameters against live performance of the current version of your strategy, so that realized and backtested PnL of your current strategy overlap as much as possible.
- Include borrow costs. As part of the effort to align production and simulated PnL, one should take borrow costs ${ }^{4}$ for shorted securities into account, since they can have a material impact on the profitability of the short side. This has challenges. Historical borrow rates are not readily available historically. The researcher may have to approximate them, or predict them on the basis of security characteristics. Another complication, albeit less impactful, is the tax treatment of dividends. When they are received by the investor, they are subject to taxation. When the investor is short the security, the treatment of dividends is more complex. In practice, tax dividend treatment is complex, and does not make a material difference in backtests, so it is not modeled. Still, be aware of it, in case you see discrepancies between accounting and simulated PnL.
- Define the backtesting protocol beforehand. A backtesting protocol is the sequence of actions and decisions that lead to assessing the performance of a strategy. It is the subject of the next section. For the sake of this list of folk precepts, it is sufficient to say that the backtesting protocol should be changed a) rarely, b) for a good reason, and c) if it changes, you should rerun and re-evaluate all your strategies under the new protocol.
- Define the dataset being used beforehand. If dataset selection is seen as part of the backtesting protocol, the heuristic follows from the previous point. The difference however is that new data become available every day, both in the form of live data, and of extensions to historical dataset. Researchers may be prone to include datasets that confirm their findings, and ignore those that do not. Ignoring new data would be suboptimal, and including them selectively may lead to the wrong conclusions. Use your judgement and research integrity, which no theorems can help.


\footnotetext{
${ }^{2}$ Although, you may argue, this whole book is an exposition of my investment philosophy. Point taken, to an extent. I am providing some building blocks, and you are reshaping and assembling them into something sensible.
${ }^{3}$ The Information Coefficient is formally defined in Section 8.3.1.
${ }^{4}$ In order to short a security, the investor (or an agent on their behalf, like a broker-dealer) must borrow it first from a lender, who charges interest on the loan.
}

\section*{8.2. The Backtesting Protocol}

\subsection*{8.2.1. Cross-Validation and Walk-Forward}

Evaluating trading strategies bears similarities with statistical model selection [Hastie et al., 2008]. We have a family of strategies (in Statistics, a family of models), and a performance measure, such as Sharpe Ratio or return on GMV. The strategies themselves may depend on several parameters. Two evaluation schemes are most common. The first one is cross-validation (Hastie et al. [2008], Ch. 7, and Mohri et al. [2018], Ch. 4). The available data is split into a training dataset and a holdout (or validation) dataset. Sometimes, based on the estimated time-dependence in the time series, the training and
holdout samples are separated by a "buffer" dataset. The training dataset is split into $K$ equal-sized samples ("folds"). Similarly to the buffer between training and holdout datasets, we may want to separate the folds by a short buffer (for equities, just one or two days) to eliminate dependencies between folds. This eliminates a possible source of look-ahead bias. Then, we perform $K$ estimation-evaluation exercises. The parameters are estimated on each of the possible combinations of $K-1$ folds, and the performance of the model is evaluated of the remaining fold using the optimized parameter; see Figure 8.1. Then estimate the cross-validation performance as the average of the single-run performances. Finally, performance is checked against the holdout sample; a scheme is shown in Figure 8.2. There are several contraindications to using crossvalidation for financial applications. First, the samples are not independent. The time dependence is reflected in the returns themselves. We know that serial dependence of returns is weak and has short memory, while volatility dependence is strong and has long memory. For some time series, it is possible to remedy this by keeping the order intact in the training folds and the errors are serially uncorrelated [Bergmeir et al., 2018, Cerqueira et al., 2023]. This is not the only issue faced in financial applications, however. For example, consider the inclusion of security momentum as a predictor. This characteristic uses past returns. Now, if the validation fold precedes temporally the training fold, these past returns are in the validation fold and we are incurring a typical instance of data leakage: the predictors directly contain information about the target. This is an obvious example, but there are subtle ones as well. For example, we could use forward earnings forecast as a predictor. But forward earnings are usually produced by analysts, who base their judgement on past returns. Like momentum, we may have leaked target data into the training set. Besides the temporal dependencies, there is another practical objection to $K$-fold crossvalidation. In their influential book, Hastie et al. [2008] (Section 7.10.2) make a forceful case that the model should entirely be selected by cross-validation. Predictive variables (be they alphas or factors, in our framework) should not be screened in advance. This is often not in practice: the predictiveness of signals or fully-fledged strategies is tested separately. Perform cross-validation enough times on different classes of models, and you will inevitably obtain favorable results. The holdout dataset is meant to serve as a final check against this "fishing expedition" [Cochrane, 2005]. Yet, when the number of raw signals runs in the millions, it is inevitable to cycle through several refinements and model revisions, so that the holdout sample performance becomes just another variable to be optimized, instead of a performance check to be run only once.

![](https://cdn.mathpix.com/cropped/2024_12_20_83bccf3336bb2a206831g-285.jpg?height=491&width=1291&top_left_y=584&top_left_x=368)

Figure 8.1: A scheme of the cross-validation procedure. Darker boxes are validation folds, while lighter boxes are training folds.

![](https://cdn.mathpix.com/cropped/2024_12_20_83bccf3336bb2a206831g-286.jpg?height=524&width=958&top_left_y=470&top_left_x=535)

Figure 8.2: A scheme of the cross-validation procedure. Data are split into two sets. Cross-validation is performed on the first one (training dataset), to estimate the expected performance of a strategy. The model is then optimized on the entire training dataset, and validated on the second one (validation dataset).

![](https://cdn.mathpix.com/cropped/2024_12_20_83bccf3336bb2a206831g-287.jpg?height=454&width=1194&top_left_y=494&top_left_x=395)

Figure 8.3: Cross-validated Sharpe for (a) Scenario 1, (b) Scenario 2.

An example may help illustrate the perils of cross-validation. We have $N=1000$ assets. We simulate iid asset returns with $r_{t, i} \sim N\left(0, \sigma^{2}\right)$, with $\sigma=0.01$. We introduce $p$ random asset characteristics, also iid drawn at random: $[\mathbf{B}]_{i, j} \sim N(0,1)$. These random features are by design not predictive of returns. The backtest consists in a 5 -fold cross-validation, to estimate the performance of the predictors. In each run, we select the best performing factor, based on in-sample IC, and then compute the IC on the test fold. Then we report the average cross-validated IC. We repeat the process on 1000 simulated datasets. Below are the results for two scenarios:
1. The first one is the "many periods, few predictors" case: we set $T=5000$ (twenty years of daily data) and $p=2$; two predictors because one would have felt too lonely.
2. The second one is the "not many periods, more predictors" case: we set $T=1250$ (five years of daily data) and $p=500$; not nearly as many as we meet in practice.

The frequency histograms of the simulations are shown in Figure 8.3. Some summary statistics of the simulations are shown in Table 8.1. The averages are close to zero in both case, with a much larger standard deviation for the many factor-case. The percentage of samples whose Sharpe Ratio passes the $1 \%$ significant level is shown in the last column of the table ${ }^{5}$. Frequency histograms for the two simulated scenarios; the conversion IC to Sharpe Ratio is $\mathrm{SR}=\mathrm{IC} \sqrt{251 N}$

Table 8.1: Frequency histograms for the two simulated scenarios; the conversion IC to Sharpe Ratio is $\mathrm{SR}=\mathrm{IC} \sqrt{251 N}$..
\begin{tabular}{lllll}
\hline$T$ & $p$ & Mean(SR) & Stdev (SR) & \% passing \\
\hline 5000 & 2 & 0.07 & 0.6 & 1.2 \\
1250 & 500 & 0.04 & 1.4 & 19 \\
\hline
\end{tabular}

A remedy to the data leakage issues arising in cross-validation is walkforward backtesting [Pardo, 2007]. In this scheme, we use historical data up to period $t$ and target returns for period $t+1$. The scheme is as close as possible to the production process. It addresses two drawbacks of cross-validation for time-series - serial dependence and risk of data leakage - and it also augments naturally the dataset with the arrival of new data. Finally, it is naturally adaptive: it fine-tunes parameters as the environment changes. These advantages are complementary to cross-validation. As a result, it is often the case that signals, or simplified strategies, are first tested using cross-validation, and then tested "out of sample" in a walk-forward test. However, this is not ideal, however, since it has an opportunity cost caused by the delay in running the strategy in production. Walk-forward has an additional important drawback: it uses less training data than cross-validation. When the set of models and parameters is very large, this limitation could be very severe. On the other side, when the model has been identified, and only a few parameters need to be optimized, then this drawback becomes negligible. Two additional trading settings in which walk-forward does not suffer from data limitations are when a) data are plenty. This is the case of high-frequency trading; b) data are very non-stationary.

\footnotetext{
${ }^{5}$ This is the percentage of simulation samples for which the condition $\mathrm{SR}>$ $2.3 \sqrt{\left(1+\mathrm{SR}^{2} / 2\right) / T}$
}
![](https://cdn.mathpix.com/cropped/2024_12_20_83bccf3336bb2a206831g-289.jpg?height=689&width=1278&top_left_y=463&top_left_x=375)

Figure 8.4: Two common walk-forward schemes. The top one uses fixed-length training data, thus keeping the estimation procedures comparable. The bottom one uses all the available data up to certain epoch, possibly weighting data differently based on the interval from the decision epoch.


This is the case, to some extent, of every trading strategy, and this very fact suggests that walk-forward backtesting is, in any event, a necessary step in the validation of a strategy, and in its preparation for production.

Summing up, neither cross-sectional nor walk-forward schemes are without flaws. Ideally, we would like a protocol with the following features.
1. non-anticipative/immune from data leakage;
2. taking into account serial dependency;
3. using all data;
4. allowing for multiple testing of a very large number of signals;
5. Providing a rigorous decision rule.

Walk forward meets the first two requirements; cross-validation meets the third. Neither meet the last two. The next section introduces a novel backtesting protocol, the Rademacher Anti-Serum (in short, RAS) (in short, RAS), which meets these requirements.

\section*{8.3. The Rademacher Anti-Serum (RAS)}

\subsection*{8.3.1. Setup}

We will be concerned with testing the performance of strategies and signals.
1. Strategies are the time series of the walk-forward simulated returns zscored by their predicted volatility, which we denote by $x_{t, n}$, so that their average equals the empirical sharpe ratio for strategy $n$. In this respect, the protocol is similar to walk-forward.
2. When we test signals, we instead consider the Information Coefficient
(IC) for the signal $n$ at time $t$, which is defined as the cosine of the angle (their cosine similarity) between the alpha vector predicted by signal $n$ in period $t$, and the idiosyncratic returns in the same period. ${ }^{6}$

The definitions are below ${ }^{7}$ :
$$
\begin{aligned}
& x_{t, n}:=\frac{\mathbf{w}_{t, n}^{\top} \mathbf{r}_{t}}{\sqrt{\mathbf{w}_{t, n}^{\top} \boldsymbol{\Omega}_{t} \mathbf{w}_{t, n}}} \\
& x_{t, n}:=\frac{\boldsymbol{\alpha}_{t, n}^{\top} \boldsymbol{\epsilon}_{t}}{\left\|\boldsymbol{\alpha}_{t, n}\right\|\left\|\boldsymbol{\epsilon}_{t}\right\|} \quad \text { (Sharpe Ratio) } \quad \text { (Information Coefficient) }
\end{aligned}
$$

We also denote $x_{t, n}$ both instances; the interpretation will be clear from the context. In either case, the dataset needed for the analysis is a $T \times N$ matrix X. Rows denote observations as of a certain timestamp and columns denote strategies, whose set we denote $S$. For notational simplicity, the $t$ th row of $\mathbf{X}$ is denoted by $\mathbf{x}_{t}$, and the $n$th column $\mathbf{x}^{n}$. In the following we make the important assumption that the random vectors $\mathbf{x}_{t}$ are iid, drawn from a common probability distribution $P$. We have two justifications for the assumptions. The first one is empirical. Serial dependence is small for returns observed at daily frequencies or lower ${ }^{8}$. The second one is that our framework can be extended to the case of time-dependent returns, at the price of weaker, asymptotic results. We recommend to inspect the autocorrelation plots of the univariate series $\mathbf{x}_{t}$. If there is sizable autocorrelation up to lag $s$, then replace the original time series with $\lfloor N / s\rfloor$ non-overlapping, contiguous averages of blocks $\left(x_{1+k s, n}, \ldots, x_{(k+1) s, n}\right)$. We employ the following notation. We let the joint distribution of $\mathbf{x}_{t}$ be $P$. Let $D=\bigotimes_{i=1}^{T} P$ be the joint probability distribution on the space of $T \times N$ matrices in which the element $\mathbf{x}_{t} \sim P$ has independent, identically distributed (iid) rows, each drawn from $P$.

\footnotetext{
${ }^{6}$ The IC is featured prominently in Section 9.5.
${ }^{7}$ For definitions and uses of $\boldsymbol{\alpha}$, see Sections 4.3 and 9.4.
${ }^{8}$ See Chapter 2 and references therein, for example Cont [2001] and Taylor [2007].
}

The expected value of $\mathbf{x}_{t}$ is denoted by $\boldsymbol{\theta} \in \mathbb{R}^{N}$. This is the true vector of strategy/signal performances. Define $\hat{\boldsymbol{\theta}}(\mathbf{X}) \in \mathbb{R}^{N}$ as the vector of column averages of $\mathbf{X}$ :
$$
\begin{equation*}
\hat{\boldsymbol{\theta}}(\mathbf{X})=\frac{1}{T} \sum_{t=1}^{T} \mathbf{x}_{t} \tag{8.1}
\end{equation*}
$$
which is the expected value of the row of $\mathbf{X}$ according to the bootstrap distribution.

Let a Rademacher random vector $\boldsymbol{\epsilon}$ be a $T$-dimensional random vector whose elements are iid and take values 1 or -1 with probability $1 / 2$. The Rademacher Complexity of $\mathbf{X}$ is defined as [Mohri et al., 2018]:
$$
\hat{R}=E_{\boldsymbol{\epsilon}}\left(\sup _{n} \frac{\boldsymbol{\epsilon}^{\top} \mathbf{x}^{n}}{T}\right)
$$

Before stating a rigorous result linking this quantity to a bound on performance, we focus our attention on its interpretation. Specifically, we can interpret $\hat{R}$ in at least three ways.
- As the covariance to random noise: Consider $\boldsymbol{\epsilon}$ as a random covariate. We can interpret $\hat{R}$ as the expected value of the highest covariance of the performance measure of a strategy to random noise. If, on average, for every set of $+/-1$ indicators, there is at least a strategy that covaries with it, then "we can do no wrong": for every realization of a random vector $\boldsymbol{\epsilon}$, there is a strategy $\mathbf{x}^{n}$ that would do well matching it, i.e., $\boldsymbol{\epsilon}^{\top} \mathbf{x}^{n} / T \simeq 1$. If we interpret the $x_{t, n}$ as predictions for epoch $t$, then this means that for every sequence of events $\epsilon_{t}$ we have a strategy that predicts them well.
- As generalized 2-way cross-validation: For sufficiently large $T$, the sets of positive elements in $\boldsymbol{\epsilon}_{t}$ concentrates around size $T / 2$. We denote $S^{+}$the set of $T / 2$ periods where $\epsilon_{t}=1$, and $S^{-}$the other periods. Rewrite the term inside the sup as
$$
\begin{aligned}
\frac{\epsilon^{\top} \mathbf{x}^{n}}{T} & =\frac{1}{2} \frac{2}{T} \sum_{s \in S^{+}} x_{s, n}-\frac{1}{2} \frac{2}{T} \sum_{s \in S^{-}} x_{s, n}=\frac{1}{2}\left(\hat{\theta}_{n}^{+}-\hat{\theta}_{n}^{-}\right) \\
\hat{\theta}_{n}^{+} & :=\frac{2}{T} \sum_{s \in S^{+}} x_{s, n} \\
\hat{\theta}_{n}^{-} & :=\frac{2}{T} \sum_{s \in S^{-}} x_{s, n}
\end{aligned}
$$

For strategy $n$, this is the discrepancy in average performance measured on two equal-sized random subsets of the observations. By taking the sup across strategies, we are estimating the worst case: we estimate performance on a subset, and get a very different result on the remaining subset! And if the discrepancy is high for each random subset, this will indicate that performance is not consistent: there's always at least a strategy that performs comparatively well somewhere and poorly in the remaining periods. The associated $\hat{R}$ is high, and means that the set of strategies has unreliable performance.
- As measure of span over possible performances: We interpret $\boldsymbol{\epsilon}$ as a "random direction" chosen at random in $\mathbb{R}^{T}$. The vector has Euclidean norm equal to $\sqrt{T}$. In the case where the performance measure is the standardized return, $E\left(\left\|\mathbf{x}^{n}\right\|\right)$ is also equal to $\sqrt{T}$, and is strongly concentrated around this value. The empirical Rademacher $\hat{R}$ is then approximately equal to
$$
E_{\epsilon}\left(\sup _{n} \frac{\epsilon^{\top} \mathbf{x}^{n}}{\|\boldsymbol{\epsilon}\|\left\|\mathbf{x}^{n}\right\|}\right)
$$

This can be interpreted in the following way. We have a set of $N$ vectors $\mathrm{x}^{1}, \ldots, \mathrm{x}^{N}$. We pick a random direction in the ambient space, and observe the maximum collinearity (expressed as the cosine similarity) of this random direction to our vectors. The expected value of this collinearity measures how much our set of strategy vectors span $\mathbb{R}^{T}$. If we have $n$ vectors that are copies of the same vector, the answer is: not very well. If, conversely, these vectors are all orthogonal, we have maximum collinearity. The Rademacher complexity is a geometric measure of how much the vectors $\mathbf{x}^{n}$ "span" $\mathbb{R}^{T}$.

One interesting characteristic of the Rademacher complexity is that it takes into account dependence among strategies. If for example we had a billion strategies to our set of candidate strategies, but they are all identical (hence perfectly correlated) we are not increasing the Rademacher complexity. However, if the strategies are uncorrelated from each other, then the Rademacher complexity is high, indicating higher likelihood of overfitting.

\subsection*{8.3.2. Main result and Interpretation}

The thrust of our protocol is to provide a uniform additive "haircut" (i.e., a term we subtract from the empirical performance) to the performance statistic. In other terms, for each strategy $n$ we have an empirical performance $\hat{\theta}_{n}$, by Equation (8.1). In the case of z -scored returns, this is the empirical Sharpe Ratio. Then, we can establish a probabilistic guarantee on the true Sharpe Ratio: with high probability, say, greater than $1-\delta$, the Sharpe Ratio of the strategy is greater than $\hat{\theta}_{n}$ - "haircut", where the haircut is a function of the Rademacher complexity, the number of samples $T$, and the parameter $\delta$.

Here, we describe the steps that establish a lower bound for performance.

We start with signals. In this case, we have $\left|x_{t, n}\right| \leq 1$, because the value is a correlation. For all signals, the true performance metric $\theta_{n}$ is bounded below by the empirical performance minus a haircut, with probability greater than $1-\delta$ :
$$
\begin{equation*}
\theta_{n} \geq \hat{\theta}_{n}-\underbrace{2 \hat{R}}_{\text {(data snooping) }}-\underbrace{2 \sqrt{\frac{\log (2 / \delta)}{T}}}_{\text {(estimation error) }} \tag{8.2}
\end{equation*}
$$

The result is described in Procedure 8.1.

\subsection*{Procedure 8.1: Rademacher Anti-Serum for signals}
1. Backtest all the strategies using a walk-forward procedure. Let $\mathbf{X} \in \mathbb{R}^{T \times N}$ be the matrix with Information Coefficients of strategy $n$ at time $t$.
2. Compute $\hat{\theta}(\mathbf{X})$, as defined in Equation (8.1).
3. Compute $\hat{R}(\mathbf{X})$.
4. For all $n \in 1, \ldots, N$
$$
\theta_{n} \geq \hat{\theta}_{n}-2 \hat{R}-2 \sqrt{\frac{\log (2 / \delta)}{T}}
$$
with probability greater than $1-\delta$.

Now, consider the case for Sharpe analysis. The formula is similar, but with a different estimation error.
$$
\begin{equation*}
\theta_{n} \geq \hat{\theta}_{n}-\underbrace{2 \hat{R}}_{\text {(data snooping) }}-\underbrace{3 \sqrt{\frac{2 \log (2 / \delta)}{T}}-\sqrt{\frac{2 \log (2 N / \delta)}{T}}}_{\text {(estimation error) }} \tag{8.3}
\end{equation*}
$$

The proofs are in the Appendix, Section 8.5.

\subsection*{Procedure 8.2: Rademacher Anti-Serum for Sharpe}
1. Backtest all the strategies using a walk-forward procedure. Let $\mathbf{X} \in \mathbb{R}^{T \times N}$ be the matrix with Information Ratio of strategy $n$ at time $t$.
2. Compute $\hat{\theta}(\mathbf{X})$, as defined in Equation (8.1).
3. Compute $\hat{R}(\mathbf{X})$.
4. For all $n \in 1, \ldots, N$
$$
\theta_{n} \geq \hat{\theta}_{n}-2 \hat{R}-3 \sqrt{\frac{2 \log (2 / \delta)}{T}}-\sqrt{\frac{2 \log (2 N / \delta)}{T}}
$$
with probability greater than $1-\delta$.


[WILEY EDITORS: place Figure 8.5 around here.]
![](https://cdn.mathpix.com/cropped/2024_12_20_83bccf3336bb2a206831g-297.jpg?height=735&width=1275&top_left_y=451&top_left_x=371)

Figure 8.5: Rademacher complexity for 5000 strategies, with iid Gaussian returns, and variable pairwise correlation. Estimate based on 2E4 samples.


We focus on the interpretation of the claim. The theorem states that the lower bounds on IC and Sharpe hold simultaneously at least with probability 1$\delta$. Moreover the statement holds for any finite $T$; no asymptotic approximation is involved. The true expected performance differs from the empirical performance because of two nonnegative terms:
- The first is the term $2 \hat{R}$. This is the data snooping term. The larger the number of strategies, the higher the $\hat{R}$, because sup is strictly increasing in the number of strategies. Moreover, as we discussed, the higher the dependency among strategies, the lower the $\hat{R}$. In the limit case where we test multiple replicas of the same strategy, $\hat{R}$ is zero. To provide some intuition about the behavior of Rademacher complexity, we consider a set of strategies with normally distributed returns with zero expected returns and a given pairwise correlation among strategy returns. Figure 8.5 displays $2 \hat{R}$ as a function of the pairwise correlation. The Rademacher complexity decreases in the correlation, and increases in the number of strategies. Given the data matrix $\mathbf{X}$, the quantity $\hat{R}$ is estimated via simulation. An upper bound for this quantity is given by Massart's lemma:
$$
\hat{R} \leq \frac{\sqrt{2 \log N}}{T}
$$
- The second is the estimation term. For some intuition, consider the case of $T$ iid normal random variables $\theta_{t}$ with mean 0 and unit variance. Their average $\hat{\theta}$ is distributed as a normal distribution with standard deviation $1 / \sqrt{T}$. What is the $\delta$-quantile of the distribution? There is no closed-formula for it, but we can approximate it using Equation (??). For a normal distribution with zero mean and standard deviation $1 / \sqrt{T}$, and Cumulative Distribution Function $F$,
$$
\begin{equation*}
F^{-1}(\delta) \geq-\sqrt{\frac{2 \log [1 /(2 \sqrt{2 \pi} \delta)]}{T}} \tag{8.4}
\end{equation*}
$$

This is similar, up to constants, to the estimation errors in Equations (8.2) and (8.3). In the limit $t \rightarrow \infty$ The estimation error in both procedures approaches 0 .

The estimation error above is independent of $N$ for bounded distributions and is $\propto \sqrt{\log N}$ for subGaussian ones. An argument for this dependency is the following. Consider the following special case. We have $N$ Gaussian signals with iid returns $N(0,1)$. The empirical Sharpe Ratios of the $N$ strategies are also iid, $\hat{\theta}_{n} \sim N(0,1 / T)$. It can be shown [van Handel, 2016, Kamath, 2020] that
$$
\frac{1}{\sqrt{\pi \log 2}} \sqrt{\frac{\log N}{T}} \leq E\left(\max _{i} \hat{\theta}_{i}\right) \leq 2 \sqrt{\frac{\log N}{T}}
$$

When the SR is zero, the maximum (and high quantiles) of the Sharpe Ratio grow as $\sqrt{(\log N) / T}$. In order not to have false positives, the "haircut" on the empirical SR should be equal to $\kappa \sqrt{(\log N) / T}$, for some positive $\kappa$. In addition, we should have a term that captures the tail behavior of $\max _{i} \hat{\theta}_{i}$. For large $N$, the dominant term of the estimation error in Equation (8.3) is $\sqrt{2(\log N) / T+2 \log (2 / \delta) / T}$, which is majorized by $\sqrt{2(\log N) / T}+$ $\sqrt{2 \log (2 / \delta) / T}$. The first term is the $\kappa \sqrt{\frac{\log N}{T}}$ growth term, and the second one is the term accounting for the confidence interval that we saw in Equation (8.4).

The procedure is operationally simple: simulate all possible strategies in a walk-forward manner. There should be no look-ahead bias: the strategies should be formulated without looking at the entire dataset and their parameters should be tuned based on past history only. As we mentioned in the "best practices" section, all strategies should be documented and should run in parallel to the production strategy. Then, estimate the Rademacher complexity of matrix $\mathbf{X}$ by the expectation in the definition of that statistic. The Rademacher complexity is easy to compute for millions (or more) strategies, and can be computed for even larger sets of strategies using tools from numerical analysis.
- The SAR procedure for signals uses the worst case $\left|x_{t, n}\right| \leq 1$. In practice, however, it is extremely unlikely to observe ICs close to one; IC greater than 0.1 is extremely unlikely. If we assume $\left|x_{t, n}\right| \leq \kappa<1$, and apply Theorem 8.3, the estimation term becomes smaller, by a factor $\kappa$ :
$$
\text { "estimation error" }=2 \kappa \sqrt{\frac{\log (2 / \delta)}{T}}
$$

Consider some realistic parameters: $\kappa=0.02, \delta=0.01$ and $T=2500$. Then the estimation error is about 0.002 .
- In the SAR procedure for strategies, the formula for the estimation error is a rather simple bound and the constant factor could be probably improved. For realistic parameters, the error is quite large. For example $\delta=0.01$, $T=2500$, and $N=1 E 6$, the estimation error is 0.31 , corresponding to an annualized estimation error of 5.1. This seems a loose bound, compared to the standard formula for the standard error of the Sharpe Ratio [Lo, 2002]: for a strategy with Sharpe Ratio equal to 3, the estimation error haircut is $F(\delta) \sqrt{\left(1+S R^{2} / 2\right) / T} \sqrt{251}=1.7$.
- The constant of the data snooping term is also conservative, since in the proof we rely on a chain of inequalities to obtain a bound.

I will close with the wise words of former colleague: "the path connecting theory and practice is paved with carefully tuned parameters." The best we can hope from theory is an insightful, interpretable result, which is amenable to be fine-tuned for applications by means of simulation. The bound of Procedure 8.2 will take the form ${ }^{9}$
$$
\theta_{n}-\hat{\theta}_{n} \geq-a \hat{R}-b \sqrt{\frac{2 \log (2 / \delta)}{T}}
$$
with parameters

\footnotetext{
${ }^{9}$ In the estimation error of Equation (8.3), for $\delta \geq 0.01$ and $N \leq 1 E 8$, the term dependent on $N$ is smaller than the term independent of it, i.e., $\sqrt{\frac{2 \log (2 N / \delta)}{T}} \leq 0.6 \times 33 \sqrt{\frac{2 \log (2 / \delta)}{T}}$, and is very weakly dependent on $N$.
}

\section*{8.4. Some Empirical Results}

\subsection*{8.4.1. Simulations}

Let us see how this approach performs in a simulated setting first. We first consider strategies whose returns are normally distributed, are iid (within each strategy) and are cross-sectionally pairwise correlated, with correlation $\rho$. Specifically, the return of strategy $i$ in period $t$ is given by $r_{i, t}=\rho * f_{t}+$ $\sqrt{1-\rho^{2}} * \epsilon_{i, t}$, with $\epsilon_{i, t} \sim N(0,1)$ and $f_{t} \sim N(0,1)$. We run simulations for different values of the correlation, for different numbers of strategies, and for expected returns (and therefore non-annualized Sharpe Ratios) equal to 0 and 0.1. For each simulation, we report the maximum empirical Sharpe Ratio $\max _{i} \hat{\theta}_{i}$, the Rademacher complexity, the estimation error, the percentage of detected positive strategies, i.e. of strategies for which the right-hand side of the Equation (8.2) exceeds 0, and is therefore deemed to have positive Sharpe Ratio. We also report the percentage of "Rademacher positive strategies, i.e., the strategies that exceed the data snooping haircut alone. In formula:
$$
\begin{align*}
\hat{\theta}_{n}-2 \hat{R}-2 \sqrt{\frac{\log (2 / \delta)}{T}} & >0  \tag{8.5}\\
\hat{\theta}_{n}-2 \hat{R} & >0 \tag{8.6}
\end{align*} \quad \text { ("positive") }
$$

We perform simulations with returns distributed both according to a Gaussian distribution and to a $t$ distribution with five degrees of freedom. The latter aims to approximate heavy-tailed returns. The simulation are performed for all possible combinations of the following parameters:
1. Correlation: $\rho \in\{0.2,0.8\}$.
2. Number of strategies: $N \in\{500,5000\}$.
3. Number of periods: $T \in\{2500,5000\}$.
4. Population Sharpe: We consider two cases. In the first one, all strategies have $\mathrm{SR}=0$. In the second one, $80 \%$ have $\mathrm{SR}=0$, and $20 \%$ have $\mathrm{SR}=0.2$ (the Sharpe Ratio is not annualized).

For each of the sixteen scenarios, we run eight simulations: we generate a historical return matrix $\mathbf{X}_{n}$. For each instance we compute the following outputs:
1. $\max _{i} \hat{\theta}_{i}^{(n)}$, the maximum realized Sharpe Ratio across the strategies.
2. $\hat{R}_{n}$ the Rademacher complexity of $\mathbf{X}_{n}$.
3. The estimation error $2 \sqrt{\log (2 / \delta) / T}$, for $\delta=5 E=2$.
4. The percentage of positive strategies, as per Equation (8.5).
5. The percentage of Rademacher positive strategies, as per Equation (8.6).
6. The percentage of true positive strategies. This percentage is either 0 or 20\%.

The tables below report the average across the eight simulations. Tables 8.3 and 8.4 show the results for normally- and t-distributed returns, respectively. 

[WILEY EDITORS: place Table 8.2 around here.]

Table 8.2: $\quad$ Comparison of $\hat{R}$ and Massart's bound.
\begin{tabular}{rrrr}
\hline$N$ & $T$ & $\hat{R}$ & Massart's bound \\
\hline 500 & 2500 & 0.059 & 0.070 \\
5000 & 2500 & 0.072 & 0.083 \\
500 & 5000 & 0.042 & 0.049 \\
5000 & 5000 & 0.051 & 0.058 \\
\hline
\end{tabular}

[WILEY EDITORS: place Table 8.3 around here.] [WILEY EDITORS: place Table 8.4 around here.]

Table 8.3: Simulations for normally distributed returns.
\begin{tabular}{rrrrrrrrrrr}
\hline$\rho$ & $N$ & $T$ & $\max _{i} \theta_{i}$ & $\max _{i} \bar{\theta}_{i}$ & $R$ & error & \% pos & \% rad pos & \% true pos \\
\hline 0.2 & 500 & 2500 & 0 & 0.059 & 0.059 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.8 & 500 & 2500 & 0 & 0.050 & 0.037 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.2 & 500 & 2500 & 0.2 & 0.252 & 0.060 & 0.109 & 1.7 & 20.0 & 20.0 \\
0.8 & 500 & 2500 & 0.2 & 0.228 & 0.037 & 0.109 & 14.3 & 20.0 & 20.0 \\
0.2 & 5000 & 2500 & 0 & 0.074 & 0.072 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.8 & 5000 & 2500 & 0 & 0.041 & 0.044 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.2 & 5000 & 2500 & 0.2 & 0.264 & 0.072 & 0.109 & 0.1 & 19.9 & 20.0 \\
0.8 & 5000 & 2500 & 0.2 & 0.242 & 0.045 & 0.109 & 12.3 & 20.0 & 20.0 \\
0.2 & 500 & 5000 & 0 & 0.039 & 0.042 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.8 & 500 & 5000 & 0 & 0.019 & 0.026 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.2 & 500 & 5000 & 0.2 & 0.234 & 0.042 & 0.077 & 19.9 & 20.0 & 20.0 \\
0.8 & 500 & 5000 & 0.2 & 0.221 & 0.026 & 0.077 & 20.0 & 20.0 & 20.0 \\
0.2 & 5000 & 5000 & 0 & 0.051 & 0.051 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.8 & 5000 & 5000 & 0 & 0.037 & 0.031 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.2 & 5000 & 5000 & 0.2 & 0.247 & 0.051 & 0.077 & 18.6 & 20.0 & 20.0 \\
0.8 & 5000 & 5000 & 0.2 & 0.230 & 0.032 & 0.077 & 20.0 & 20.0 & 20.0 \\
\hline
\end{tabular}


Table 8.4: Simulations for t-distributed returns.
\begin{tabular}{rrrrrrrrrrr}
\hline$\rho$ & $N$ & $T$ & $\max _{i} \theta_{i}$ & $\max _{i} \bar{\theta}_{i}$ & $R$ & error & \% pos & \% rad pos & \% true pos \\
\hline 0.2 & 500.0 & 2500 & 0 & 0.059 & 0.059 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.8 & 500.0 & 2500 & 0 & 0.035 & 0.036 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.2 & 500.0 & 2500 & 0.2 & 0.250 & 0.060 & 0.109 & 1.4 & 20.0 & 20.0 \\
0.8 & 500.0 & 2500 & 0.2 & 0.237 & 0.037 & 0.109 & 16.6 & 20.0 & 20.0 \\
0.2 & 5000.0 & 2500 & 0 & 0.070 & 0.072 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.8 & 5000.0 & 2500 & 0 & 0.045 & 0.044 & 0.109 & 0.0 & 0.0 & 0.0 \\
0.2 & 5000.0 & 2500 & 0.2 & 0.265 & 0.072 & 0.109 & 0.1 & 19.9 & 20.0 \\
0.8 & 5000.0 & 2500 & 0.2 & 0.242 & 0.045 & 0.109 & 11.0 & 20.0 & 20.0 \\
0.2 & 500.0 & 5000 & 0 & 0.047 & 0.042 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.8 & 500.0 & 5000 & 0 & 0.023 & 0.026 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.2 & 500.0 & 5000 & 0.2 & 0.234 & 0.042 & 0.077 & 20.0 & 20.0 & 20.0 \\
0.8 & 500.0 & 5000 & 0.2 & 0.220 & 0.026 & 0.077 & 20.0 & 20.0 & 20.0 \\
0.2 & 5000.0 & 5000 & 0 & 0.051 & 0.051 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.8 & 5000.0 & 5000 & 0 & 0.034 & 0.031 & 0.077 & 0.0 & 0.0 & 0.0 \\
0.2 & 5000.0 & 5000 & 0.2 & 0.247 & 0.051 & 0.077 & 18.5 & 20.0 & 20.0 \\
0.8 & 5000.0 & 5000 & 0.2 & 0.231 & 0.031 & 0.077 & 20.0 & 20.0 & 20.0 \\
\hline
\end{tabular}

We interpret the results below.

1. For a fixed distribution of the population Sharpe, the maximum empirical Sharpe Ratio $\max _{i} \bar{\theta}_{i}$ is predictably increasing in $N$, it is decreasing in $\rho$ because the "effective" number of strategies decreases, as the strategies are more correlated, and it is decreasing in $T$, by the Central Limit Theorem.
2. Everything else equal, the Rademacher complexity is decreasing in $\rho$, increasing in $N$, and is decreasing in $T$. It is interesting to compare $\hat{R}$ to the bound from Massart's lemma. in Table 8.2, I report the highest $\hat{R}$ from Table 8.3, and Massart's bounds. Massart's bound is at most $19 \%$ higher than the observed Rademacher complexity.   
3. The data snooping term and the estimation error term have the same magnitude.
4. In the null Sharpe case (all strategies have zero Sharpe Ratio), the
percentage of detected positive cases ("\% pos") is zero or nearly zero in all cases; there are no false positives.
5. In the alternative Sharpe case ( $20 \%$ of strategies have Sharpe Ratio equal to 0.2 ), the percentage positives is always smaller than the percentage of true positives. All detected positives are in fact true positives: the False Discovery Rate (FDR), defined as the ratio of false positives to all detected positives, is zero. In a few cases the percentage of positives is much lower than the percentage of true positives. The bounds is too conservative.
6. The percentage of Rademacher positives is much closer to the percentage of true positives. The FDR is still zero in this case.


\subsection*{8.4.2. Historical Anomalies}

It is of independent interest to analyze anomalies published in the academic literature. We consider two sources of returns. In their paper, Jensen et al. [2023] extend a dataset of factor anomalies introduced by Hou et al. [2020], and test their replicability and out-of-sample performance. The dataset contains published characteristics, and it is not possible to know which characteristics were tested prior to publications. In principle, all such variants should be included in the study. Had they been included, the Rademacher complexity of the dataset would have been higher. As criteria for inclusion, we required that the factors have at least ten years of trading history, and that they be produced on the last day ${ }^{10}$ in the dataset, December 31. 2023. We perform the analysis at the country level. Table 8.5 shows the same columns as the previous two table, with the exception of the percentage of true positives, which is not know a priori. The United Kingdom, United States and Hong Kong and Malaysia are the only markets where a positive percentage of factors meet the Rademacher bound, Equation 8.6.

[WILEY EDITORS: place Table 8.5 around here.] 

Table 8.5: $\quad$ Summary data for the factors in Jensen et al.'s database.
\begin{tabular}{lrrrrrrr}
\hline Country & $N$ & $T$ & $\max _{i} \bar{\theta}_{i}$ & $\hat{R}$ & error & $\%$ pos & $\% \mathrm{rad}$ pos \\
\hline AUS & 153 & 3584 & 0.072 & 0.040 & 0.091 & 0.0 & 0.0 \\
BRA & 141 & 2823 & 0.057 & 0.046 & 0.102 & 0.0 & 0.0 \\
CAN & 153 & 3880 & 0.043 & 0.038 & 0.087 & 0.0 & 0.0 \\
CHE & 153 & 3358 & 0.066 & 0.042 & 0.094 & 0.0 & 0.0 \\
DEU & 153 & 4475 & 0.052 & 0.036 & 0.081 & 0.0 & 0.0 \\
FRA & 153 & 4327 & 0.055 & 0.037 & 0.083 & 0.0 & 0.0 \\
GBR & 153 & 4546 & 0.082 & 0.036 & 0.081 & 0.0 & $\mathbf{0 . 7}$ \\
HKG & 153 & 4101 & 0.100 & 0.037 & 0.085 & 0.0 & $\mathbf{6 . 5}$ \\
IND & 153 & 2785 & 0.091 & 0.046 & 0.103 & 0.0 & 0.0 \\
JPN & 150 & 2549 & 0.036 & 0.047 & 0.108 & 0.0 & 0.0 \\
KOR & 148 & 2560 & 0.049 & 0.047 & 0.107 & 0.0 & 0.0 \\
MYS & 152 & 3436 & 0.084 & 0.042 & 0.093 & 0.0 & $\mathbf{0 . 7}$ \\
SAU & 139 & 2925 & 0.076 & 0.045 & 0.100 & 0.0 & 0.0 \\
THA & 146 & 2988 & 0.043 & 0.044 & 0.099 & 0.0 & 0.0 \\
TWN & 151 & 2899 & 0.068 & 0.044 & 0.101 & 0.0 & 0.0 \\
USA & 153 & 13155 & 0.069 & 0.020 & 0.047 & 0.0 & $\mathbf{1 8 . 3}$ \\
ZAF & 151 & 2676 & 0.062 & 0.047 & 0.105 & 0.0 & 0.0 \\
\hline
\end{tabular}

Another source of factor return data is curated by Andrew Chen [Chen and Zimmerman, 2022]. Among the anomalies ${ }^{11}$, we select those that were available as of the end of
2023 and had at least five years of history. The summary results are displayed in Table 8.6. The percentage of strategies that meet the Rademacher bound is above $3 \%$. The smaller percentage is attributable to the fact that the number of periods (days) with complete observations is 5911 (compared to 13155 for the Jensen, Kelly and Pedersen dataset) and to the Rademacher complexity being 0.033 (compared to 0.02 for the Jensen, Kelly and Pedersen dataset).

[WILEY EDITORS: place Table 8.6 around here.]

Table 8.6: Summary data for the factors in Zimmerman and Chen's database.
\begin{tabular}{rrrrrrr}
\hline$N$ & $T$ & $\max _{i} \bar{\theta}_{i}$ & $\hat{R}$ & error & \% pos & \% rad pos \\
\hline 192 & 5911 & 0.126 & 0.033 & 0.070 & 0 & $\mathbf{3 . 1}$ \\
\hline
\end{tabular}


\footnotetext{
${ }^{10}$ Data downloaded on August 15, 2024 from https://jkpfactors.com/.
${ }^{11}$ We use the 'Predictor' dataset, downloaded from https://www.openassetpricing/ on
}


\section*{8.5. Appendix}

\subsection*{8.5.1. Proofs for RAS}

We use some essential inequalities in the proofs. Standard references are Boucheron et al. [2013] and Vershynin [2018].

Theorem 8.1(McDiarmid's inequality): Let $X_{1}, \ldots, X_{n}$ be independent random variables, and $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$, such that, for each $i$,
$$
\sup _{x_{i}, x_{i}^{\prime}}\left|f\left(x_{1}, \ldots, x_{i}, \ldots, x_{n}\right)-f\left(x_{1}, \ldots, x_{i}^{\prime}, \ldots, x_{n}\right)\right| \leq c_{i}
$$

Then, for all $\epsilon>0$,
$$
P(|f-E f|>\epsilon) \leq \exp \left(-\frac{2 \epsilon^{2}}{\sum_{i} c_{i}^{2}}\right)
$$

Specifically, if $c_{i}=c$, and with probability greater than $1-\delta / 2$,
$$
f<E f-\sqrt{\frac{n c^{2}}{2} \log (\delta / 2)}
$$

A mean-zero $\sigma$-Sub-Gaussian random variable $X$ is one for which a positive constant $\sigma$ exists, such that the inequality $P(|X|>\epsilon) \leq 2 \exp \left(-\epsilon^{2} /\left(2 \sigma^{2}\right)\right)$ holds for all positive $\epsilon$. The parameter $\sigma^{2}$ is the proxy variance.

Theorem 8.2(Generalized Hoeffding's inequality): Let $X_{1}, \ldots, X_{n}$ be i.i.d. random variables with finite sub-Gaussian norms and proxy. Then, for all $\epsilon>0$,
$$
\begin{equation*}
P\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}-E X>\epsilon\right) \leq \exp \left(-\frac{n \epsilon^{2}}{2 \sigma^{2}}\right) \tag{8.7}
\end{equation*}
$$

Theorem 8.3(Bounds for Bounded Performance Metrics): Assume that $\left|x_{t n}\right| \leq a$ for all $n=1, \ldots, N, t=1, \ldots, T$. For all $n \in 1, \ldots, N$
$$
\begin{equation*}
\theta_{n} \geq \hat{\theta}_{n}-2 \hat{R}-3 a \sqrt{\frac{2 \log (2 / \delta)}{T}} \tag{8.8}
\end{equation*}
$$

Proof. The straightforward inequality holds for all $n=1, \ldots, N: \theta_{n}-\hat{\theta}_{n} \geq$ $-\sup _{n}\left|\hat{\theta}_{n}-\theta_{n}\right|$. Define
$$
\begin{equation*}
\Phi:=\sup _{n}\left|\hat{\theta}_{n}-\theta_{n}\right| \tag{8.9}
\end{equation*}
$$

We claim that with probability greater than $1-\delta / 2$
$$
\begin{equation*}
\Phi \leq E_{D} \Phi+a \sqrt{\frac{2 \log (2 / \delta)}{T}} \tag{8.10}
\end{equation*}
$$

This allows one to deal with $E_{D} \sup _{n}\left|\hat{\theta}_{n}-\theta_{n}\right|$, which is easier. To prove the inequality, note that, for all $x_{t, i}, x_{t, i}^{\prime} \in[-a, a], t=1, \ldots, T, i=1, \ldots, N$,
$$
\begin{equation*}
\left|\hat{\theta}_{n}\left(\ldots, x_{t, i}, \ldots\right)-\hat{\theta}_{n}\left(\ldots, x_{t, i}^{\prime}, \ldots\right)\right| \leq \frac{2 a}{T} \tag{8.11}
\end{equation*}
$$
from which it follows that
$$
\sup _{x_{t, n}, x_{t, n}^{\prime} \in \mathbb{R}^{T}}\left|\Phi\left(\ldots, x_{t, n}, \ldots\right)-\Phi\left(\ldots, x_{t, n}^{\prime}, \ldots\right)\right| \leq \frac{2 a}{T}
$$

We apply McDiarmid's inequality to $\Phi$ to obtain the result.
In order to obtain a lower bound on $\theta_{n}$ we need an upper bound on $E_{D} \Phi$. In the equalities below, we introduce a probability measure $D^{\prime}$ identical to, and independent from, $D$.
$$
\begin{aligned}
& E_{D} \sup _{n}\left|\hat{\theta}_{n}-\theta_{n}\right| \\
& =E_{D} \sup _{n}\left|\hat{\theta}_{n}(\omega)-E_{D^{\prime}} \hat{\theta}_{n}\left(\omega^{\prime}\right)\right| \\
& =E_{D} \sup _{n}\left|E_{D^{\prime}} \hat{\theta}_{n}(\omega)-E_{D^{\prime}} \hat{\theta}_{n}\left(\omega^{\prime}\right)\right| \quad \text { (conditioning) } \\
& \leq E_{D} E_{D^{\prime}} \sup _{n}\left|\hat{\theta}_{n}(\omega)-\hat{\theta}_{n}\left(\omega^{\prime}\right)\right| \\
& \leq \frac{1}{T} E_{D} E_{D^{\prime}} \sup _{n}\left|\sum_{t}\left(x_{t, n}(\omega)-x_{t, n}\left(\omega^{\prime}\right)\right)\right| \\
& =(*)
\end{aligned}
$$

We introduce an additional source of noise (the $\boldsymbol{\epsilon}$ Rademacher vector) and we lose a constant of 2 , but gain in tractability. We can change the signs of each summand by multiplying by some arbitrary factor $y_{t} \in\{+1,-1\}$, since the
terms are exchangeable.
$$
\begin{aligned}
(*) & =\frac{1}{T} E_{D} E_{D^{\prime}} \sup _{n}\left|\sum_{t} y_{t}\left(x_{t, n}(\omega)-x_{t, n}\left(\omega^{\prime}\right)\right)\right| \\
& =\frac{1}{T} E_{D} E_{D^{\prime}} E_{\epsilon} \sup _{n}\left|\sum_{t} \epsilon_{t}\left(x_{t, n}(\omega)-x_{t, n}\left(\omega^{\prime}\right)\right)\right| \\
& \leq \frac{1}{T} E_{D} E_{D^{\prime}} E_{\epsilon} \sup _{n} \left\lvert\, \sum_{t} \epsilon_{t}\left(\left.x_{t, n}(\omega)\left|+\frac{1}{T} E_{D} E_{D^{\prime}} E_{\epsilon} \sup _{n}\right| \sum_{t} \epsilon_{t} x_{t, n}\left(\omega^{\prime}\right) \right\rvert\,\right.\right. \\
& =\frac{1}{T} E_{D} E_{\epsilon} \sup _{n}\left|\sum_{t} \epsilon_{t} x_{t, n}(\omega)\right|+\frac{1}{T} E_{D} E_{\epsilon} \sup _{n}\left|\sum_{t} \epsilon_{t} x_{t, n}(\omega)\right| \\
& =\frac{2}{T} E_{D} \hat{R} \\
& =2 R
\end{aligned}
$$

Where we defined $R$ as the expected value of the Rademacher complexity over the distribution of performance realizations.

We now use McDiarmid again: for all $x_{t, i}, x_{t, i}^{\prime}$,
$$
\left|\hat{R}\left(\ldots, x_{t, i}, \ldots\right)-\hat{R}\left(\ldots, x_{t, i}^{\prime}, \ldots\right)\right| \leq \frac{2 a}{T}
$$

Hence, with probability greater than $1-\delta / 2$
$$
\begin{equation*}
R \leq \hat{R}+a \sqrt{\frac{2 \log (2 / \delta)}{T}} \tag{8.12}
\end{equation*}
$$

Now we employ the union bound on inequalities (8.10) and (8.12) to obtain the claim.

Theorem 8.4(Bounds for Sub-Gaussian Performance Metrics): Assume that $P\left(\left|x_{t, n}\right|>\epsilon\right) \leq 2 e^{-\epsilon^{2} / 2}$ for all $\epsilon>0$, for all $n=1, \ldots, N, t=1, \ldots, T$.

Then, for all $n \in 1, \ldots, N$
$$
\theta_{n}-\hat{\theta}_{n} \geq-2 \hat{R}-3 \sqrt{\frac{2 \log (2 / \delta)}{T}}-\sqrt{\frac{2 \log (2 N / \delta)}{T}}
$$

Proof. Let $a>0$. We split $\theta_{n}-\hat{\theta}_{n}$ into the sum of two terms: $\theta_{n}-\hat{\theta}_{n}=$ $g\left(\mathbf{x}^{n}, a\right)+h\left(\mathbf{x}^{n}, a\right)$, where
$$
\begin{aligned}
& \theta_{n}-\hat{\theta}_{n}=g\left(\mathbf{x}^{n}, a\right)+h\left(\mathbf{x}^{n}, a\right) \\
& \geq-\sup _{n}\left|g\left(\mathbf{x}^{n}, a\right)\right|-\sup _{n}\left|h\left(\mathbf{x}^{n}, a\right)\right| \\
& g\left(\mathbf{x}^{i}, a\right):=E\left[\frac{1}{T} \sum_{t=1}^{T} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right| \leq a\right)\right]-\frac{1}{T} \sum_{t=1}^{T} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right| \leq a\right) \\
& h\left(\mathbf{x}^{i}, a\right):=E\left[\frac{1}{T} \sum_{t=1}^{T} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right|>a\right)\right]-\frac{1}{T} \sum_{t=1}^{T} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right|>a\right)
\end{aligned}
$$

We bound $P\left(\sup _{i}\left|h\left(\mathbf{x}^{i}, a\right)\right| \geq v\right)$. By symmetrization
$$
E\left|h\left(\mathbf{x}^{i}, a\right)\right| \leq 2 E \sum_{t=1}^{T}\left|\epsilon_{t} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right|>a\right)\right|
$$

The random variable $\left|\epsilon_{t} x_{t, i} \mathbf{1}\left(\left|x_{t, i}\right|>a\right)\right|$ is subGaussian, since it is dominated by $\left|x_{t, i}\right|$ with probability 1 , and it has the same proxy variance as $\left|x_{t, i}\right|$. By the General Hoeffing inequality,
$$
\begin{aligned}
P\left(\left|\sum_{t=1}^{T} h\left(x_{t, i}\right)\right|>v\right) & \leq \exp \left(-T v^{2} / 2\right) \\
P\left(\sup _{i}\left|\sum_{t=1}^{T} h\left(x_{t, i}\right)\right|>v\right) & \leq N \exp \left(-T v^{2} / 2\right) \\
P\left(\sup _{i}\left|\sum_{t=1}^{T} h\left(x_{t, i}\right)\right|>\sqrt{2 \log (2 N / \delta) / T}\right) & \leq \delta / 2
\end{aligned}
$$

By the union bound,
$$
\begin{equation*}
\theta_{n}-\hat{\theta}_{n} \geq-2 \hat{R}-3 \sqrt{\frac{2 \log (2 / \delta)}{T}}-\sqrt{\frac{2 \log (2 N / \delta)}{T}} \tag{8.13}
\end{equation*}
$$
