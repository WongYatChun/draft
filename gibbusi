import numpy as np
import pandas as pd

def exponential_decay_weights(N, half_life=None, normalize='sum_to_N'):
    """
    Generate an array of weights based on exponential decay or equal weighting.

    Parameters:
    - N (int): Number of data points (must be a positive integer).
    - half_life (float or None): Half-life period (must be positive).
        If None, assigns equal weights to all data points.
    - normalize (str): Normalization method. Options:
        - 'raw': No normalization; raw exponential decay weights or equal weights if half_life is None.
        - 'sum_to_1': Normalize weights so that their sum equals 1.
        - 'sum_to_N': Normalize weights so that their sum equals N.
      Default is 'sum_to_N'.

    Returns:
    - weights (numpy.ndarray): Array of weights based on the chosen normalization.
    """
    # Input Validation
    if not isinstance(N, int) or N <= 0:
        raise ValueError("Number of data points N must be a positive integer.")
    
    if half_life is not None and half_life <= 0:
        raise ValueError("Half-life must be positive or None.")
    
    if normalize not in ['raw', 'sum_to_1', 'sum_to_N']:
        raise ValueError("normalize must be one of 'raw', 'sum_to_1', or 'sum_to_N'.")

    # Weight Generation
    if half_life is None:
        # Assign equal weights
        weights = np.ones(N)
    else:
        # Calculate the decay constant
        lambda_ = np.log(2) / half_life

        # Generate weights: w_k = e^(-lambda * k) for k = 0 to N-1
        k = np.arange(N)
        weights = np.exp(-lambda_ * k)

    # Apply normalization based on the 'normalize' parameter
    if normalize == 'sum_to_1':
        weights /= weights.sum()
    elif normalize == 'sum_to_N':
        weights *= (N / weights.sum())
    # If 'raw', do nothing

    return weights

def perform_svd(R, n_components=None):
    """
    Perform Singular Value Decomposition (SVD) and truncate to retain top components.

    Parameters:
    - R (numpy.ndarray): Input matrix of shape (n_assets, n_time_periods).
    - n_components (int): Number of principal components to retain.

    Returns:
    - U_truncated (numpy.ndarray): Truncated left singular vectors (loadings).
    - S_truncated (numpy.ndarray): Truncated singular values.
    - Vt_truncated (numpy.ndarray): Truncated right singular vectors (time factors).
    """
    U, S, Vt = np.linalg.svd(R, full_matrices=False)
    if n_components is not None:
        U = U[:, :n_components]
        S = S[:n_components]
        Vt = Vt[:n_components, :]
    return U, S, Vt

def estimate_idiosyncratic_variances(R, U_p, S_p, Vt_p):
    """
    Estimate idiosyncratic variances for each asset.

    Parameters:
    - R (numpy.ndarray): Original return matrix of shape (n_assets, n_time_periods).
    - U_p (numpy.ndarray): Truncated left singular vectors from first stage PCA (n_assets, p).
    - S_p (numpy.ndarray): Truncated singular values from first stage PCA (p,).
    - Vt_p (numpy.ndarray): Truncated right singular vectors from first stage PCA (p, n_time_periods).

    Returns:
    - sigma_squared (numpy.ndarray): Idiosyncratic variances for each asset (n_assets,).
    """
    # Reconstruct the approximated return matrix
    R_explained = U_p @ np.diag(S_p) @ Vt_p  # Shape: (n_assets, n_time_periods)

    # Compute idiosyncratic returns (residuals)
    residuals = R - R_explained  # Shape: (n_assets, n_time_periods)

    # Compute squared residuals
    residuals_squared = residuals ** 2  # Shape: (n_assets, n_time_periods)

    # Compute idiosyncratic variances as the mean of squared residuals
    sigma_squared = residuals_squared.mean(axis=1)  # Shape: (n_assets,)

    # Handle potential division by zero by adding a small epsilon where sigma_squared is zero
    epsilon = 1e-8
    sigma_squared = np.where(sigma_squared == 0, epsilon, sigma_squared)

    return sigma_squared

class FactorModelEstimator:
    def __init__(self, tau_fast, tau_slow, n_first_pca_component, n_second_pca_component):
        """
        Initialize the FactorModelEstimator.

        Parameters:
        - tau_fast (float): Fast half-life for time-series weighting.
        - tau_slow (float): Slow half-life for time-series weighting.
        - n_first_pca_component (int): Number of principal components to retain in the first stage.
        - n_second_pca_component (int): Number of final factors to retain in the second stage.
        """        
        self.tau_fast = tau_fast
        self.tau_slow = tau_slow
        self.n_first_pca_component = n_first_pca_component
        self.n_second_pca_component = n_second_pca_component
        self.intermediate = {}  # Dictionary to store all intermediate and final values

    def fit(self, R):
        """
        Fit the two-stage PCA factor model to the return matrix.

        Parameters:
        - R (numpy.ndarray): Return matrix of shape (n_assets, n_time_periods).

        Returns:
        - None
        """
        n_assets, n_periods = R.shape

        # Step 1: Generate fast and slow exponential decay weights
        weights_fast = exponential_decay_weights(N=n_periods, half_life=self.tau_fast, normalize='sum_to_N')
        weights_slow = exponential_decay_weights(N=n_periods, half_life=self.tau_slow, normalize='sum_to_N')
        self.intermediate['weights_fast'] = weights_fast
        self.intermediate['weights_slow'] = weights_slow

        # Step 2: Create diagonal weighting matrices
        W_tau_fast = np.diag(weights_fast)    # Shape: (n_periods, n_periods)
        W_tau_slow = np.diag(weights_slow)    # Shape: (n_periods, n_periods)
        self.intermediate['W_tau_fast'] = W_tau_fast
        self.intermediate['W_tau_slow'] = W_tau_slow

        # Step 3: Apply fast weighting to the return matrix (First Stage)
        tilde_R_first_pca = R @ W_tau_fast    # Shape: (n_assets, n_periods)
        self.intermediate['tilde_R_first_pca'] = tilde_R_first_pca

        # Step 4: Perform first stage PCA
        U_first_pca, S_first_pca, Vt_first_pca = perform_svd(tilde_R_first_pca, self.n_first_pca_component)
        self.intermediate['U_first_pca'] = U_first_pca
        self.intermediate['S_first_pca'] = S_first_pca
        self.intermediate['Vt_first_pca'] = Vt_first_pca

        # Step 5: Estimate idiosyncratic variances
        sigma_squared = estimate_idiosyncratic_variances(R, U_first_pca, S_first_pca, Vt_first_pca)  # Shape: (n_assets,)
        self.intermediate['sigma_squared'] = sigma_squared

        # Step 6: Create idiosyncratic variance matrix D
        D = np.diag(sigma_squared)              # Shape: (n_assets, n_assets)
        self.intermediate['D'] = D

        # Step 7: Create idiosyncratic weighting matrix W_idio = diag(1 / sigma_i)
        sigma = np.sqrt(sigma_squared)          # Shape: (n_assets,)
        sigma_inv = 1 / sigma                    # Shape: (n_assets,)
        W_idio = np.diag(sigma_inv)             # Shape: (n_assets, n_assets)
        self.intermediate['W_idio'] = W_idio

        # Step 8: Apply idiosyncratic and slow weighting to the return matrix (Second Stage)
        hat_R_second_pca = W_idio @ R @ W_tau_slow  # Shape: (n_assets, n_periods)
        self.intermediate['hat_R_second_pca'] = hat_R_second_pca

        # Step 9: Perform second stage PCA
        U_second_pca, S_second_pca, Vt_second_pca = perform_svd(hat_R_second_pca, self.n_second_pca_component)
        self.intermediate['U_second_pca'] = U_second_pca
        self.intermediate['S_second_pca'] = S_second_pca
        self.intermediate['Vt_second_pca'] = Vt_second_pca

        # Step 10: Assemble the final factor loadings matrix B = diag(sigma) @ U_second_pca
        B = np.diag(sigma) @ U_second_pca                 # Shape: (n_assets, n_second_pca_component)
        self.intermediate['B'] = B

        # Step 11: Define factor variances based on singular values
        factor_variances = S_second_pca ** 2          # Shape: (n_second_pca_component,)
        self.intermediate['factor_variances'] = factor_variances

        # Step 12: Compute lambda_bar as the average of the remaining singular values squared
        # Since we've truncated to n_second_pca_component, if there are more singular values,
        # compute the mean of their squares. Otherwise, set lambda_bar to zero.
        if self.n_second_pca_component < len(S_second_pca):
            remaining_singular_values = S_second_pca[self.n_second_pca_component:]
            lambda_bar = np.mean(remaining_singular_values ** 2)
        else:
            lambda_bar = 0  # No remaining singular values
        self.intermediate['lambda_bar'] = lambda_bar

    def get_intermediate(self, key):
        """
        Retrieve a specific intermediate value by key.

        Parameters:
        - key (str): The key corresponding to the desired intermediate value.

        Returns:
        - The requested intermediate value.

        Raises:
        - KeyError: If the key does not exist in the intermediate dictionary.
        """
        if key not in self.intermediate:
            raise KeyError(f"Intermediate value '{key}' not found.")
        return self.intermediate[key]

    def get_loadings(self):
        """
        Get the factor loadings matrix.

        Returns:
        - B (numpy.ndarray): Factor loadings matrix of shape (n_assets, n_second_pca_component).

        Raises:
        - ValueError: If the model has not been fitted yet.
        """
        if 'B' not in self.intermediate:
            raise ValueError("The model has not been fitted yet. Call the 'fit' method first.")
        return self.intermediate['B']

    def get_factor_variances(self):
        """
        Get the variances of the factors.

        Returns:
        - factor_variances (numpy.ndarray): Variances of the factors of shape (n_second_pca_component,).

        Raises:\title{Chapter 4 Linear Models of Returns}

\section*{Overview}

The Questions

1. What do we mean by linear models of returns?
2. How do we interpret them?
3. What are their applications?

Linear models of asset returns are a cornerstone of this book. They are flexible, interpretable, perform well in applications, and are supported by theory. Furthermore, they fit like a glove with mean-variance optimization and can be also used as a basis for a number of important tasks, like risk management and performance analysis. It is possible that you, the reader, will find this class of models inadequate in some way, at some point. But just because you have outgrown them does not mean that you will find them useless. They will still enable you to reason about the entire investment process, and some of the theory will come in handy.

\subsection*{4.1. Factor Models}

We saw in Chapter 2 how to model univariate returns. A direct extension to multivariate returns would be to model each security's return as an independent process. This would not be adequate, however, because the returns are dependent. It is a natural step to model the common dependency among stocks as being generated by a few common sources of randomness, called factors, and then to keep a random source of per-security randomness that is independent of the factors. The model for stock returns is called a factor model ${ }^{1}$ and takes the form:
$$
\begin{equation*}
\mathbf{r}_{t}=\boldsymbol{\alpha}+\mathbf{B f}_{t}+\boldsymbol{\epsilon}_{t} \tag{4.1}
\end{equation*}
$$
where:
- $t \in \mathbb{N}$ denotes the discrete time period.
- $\mathbf{r}_{t}$ is a random vector of $n$ asset returns minus the risk-free rate (see Section 2.1.2).
- $\boldsymbol{\alpha}$ is an $n$-dimensional vector.
- $\mathbf{f}_{t}$ is the random vector of $m$ factor returns;
- $\mathbf{B}$ is a $n \times m$ loadings matrix, with $m<n$. The matrix element $[\mathbf{B}]_{i, j}$ is the loading of asset $i$ with respect to factor $j$.
- $\epsilon_{t}$ is the random vector of $n$ idiosyncratic (or specific) returns ${ }^{2}$.

\footnotetext{
${ }^{1}$ Factor models go back to the birth of psychometrics at the turn of the 19th century. Textbooks treatments on the subject are Rencher and Christensen [2012], Johnson and Wichern [2007]. In finance, factor models were first introduced by Sharpe [1964], Sharpe [1965], and Sharpe [1966] for the one-factor case, which was extended to multiple factors by Ross [Ross, 1976]. Good introductions to factor models in finance are the survey papers by Connor and Korajczyk [2010], Fan et al. [2016], and the books Connor et al. [2010] and Connor and Korajczyk [2010], MacKinlay [1995].
${ }^{2}$ We also use the terms residual and specific in place of "idiosyncratic".
}

If the random vector $\boldsymbol{\epsilon}_{t}$ had a generic distribution, we would gain nothing in tractability. Instead, we assume that i) The vector $\boldsymbol{\epsilon}_{t}$ is independent from the factor returns $\mathbf{f}_{s}$ for all $s$; ii) $E\left[\boldsymbol{\epsilon}_{t}\right]=0$; iii) the covariance matrix $\boldsymbol{\Omega}_{\boldsymbol{\epsilon}, t}:=\operatorname{var}\left(\boldsymbol{\epsilon}_{t}\right)$ is diagonal, or at least sparse in some sense. Often, models with a diagonal covariance matrix are called strict and models with a sparse covariance matrix are called approximate. The vector $\boldsymbol{\epsilon}_{t}$ is the idiosyncratic component of asset returns.

We usually refer to the term $\mathbf{B f}_{t}$ as the systematic component of asset returns. We assume that the pair $\left(\mathbf{f}_{t}, \boldsymbol{\epsilon}_{t}\right)$ is either identically distributed across periods or has a slowly varying distribution, and that $\mathbf{f}_{t}$ and $\boldsymbol{\epsilon}_{t}$ are independent for each $t$. We denote covariance matrices of $\mathbf{f}_{t}$ and $\boldsymbol{\epsilon}_{t}$ with $\boldsymbol{\Omega}_{\mathbf{f}} \in \mathbb{R}^{m \times m}$ and $\Omega_{\epsilon} \in \mathbb{R}^{n \times n}$ respectively. With this notation, the covariance matrix of assets is
$$
\begin{equation*}
\Omega_{\mathrm{r}}=\mathrm{B} \Omega_{\mathrm{f}} \mathrm{~B}^{\top}+\Omega_{\epsilon} \tag{4.2}
\end{equation*}
$$

This decomposition is at the core of volatility modeling with linear returns and the subject of Chapters 6 and 7 .

\subsubsection*{FAQ 4.1: Why is the covariance matrix $\boldsymbol{\Omega}_{\mathbf{r}}=\mathbf{B} \boldsymbol{\Omega}_{\mathbf{f}} \mathbf{B}^{\boldsymbol{\top}}+\boldsymbol{\Omega}_{\boldsymbol{\epsilon}}$ ?}
The covariance matrix $\boldsymbol{\Omega}_{\mathrm{r}}$ does not depend on the intercept $\boldsymbol{\alpha}$, and the terms $\mathbf{B f}_{t}$ and $\boldsymbol{\epsilon}_{t}$ are independent, so that $\boldsymbol{\Omega}_{\mathrm{r}}=\operatorname{cov}\left(\mathbf{B f}_{t}\right)+\boldsymbol{\Omega}_{\boldsymbol{\epsilon}}$. The factor term is a linear transformation of $\mathbf{f}_{t}$. For any random vector $\boldsymbol{\xi}$ with covariance $\boldsymbol{\Omega}_{\boldsymbol{\xi}}, \operatorname{cov}(\mathbf{B} \boldsymbol{\xi})=\mathbf{B} \boldsymbol{\Omega}_{\boldsymbol{\xi}} \mathbf{B}^{\boldsymbol{\top}}$, because $[\operatorname{cov}(\mathbf{B} \boldsymbol{\xi})]_{i, j}=$ $\operatorname{cov}\left(\sum_{k}[\mathbf{B}]_{i, k} \xi_{k}, \sum_{\ell}[\mathbf{B}]_{j, \ell} \xi_{\ell}\right)=\sum_{k, \ell}[\mathbf{B}]_{i, k}\left[\boldsymbol{\Omega}_{\xi}\right]_{k, \ell}[\mathbf{B}]_{j, \ell}=\left[\mathbf{B} \boldsymbol{\Omega}_{\xi} \mathbf{B}^{\boldsymbol{\top}}\right]_{i, j}$.

In this chapter we set aside the very important issue of estimating the parameters of Equation (4.1) from data, and focus instead on its usage and
interpretation. Here is the plan for the next few sections. First, we review the interpretations of Equation (4.1), of which there are three:
1. as a graphical model;
2. as the superposition of low-dimensional cross-sectional return vectors;
3. as the overlap of the factor return vector with the asset loadings vector.

We then analyze the sources of alpha, namely alpha spanned and alpha orthogonal. These are fundamental concepts at the core of the research process. Third, we review the transformations that can be operated on factor models. There are three of those too:
1. Rotations keep the dimension of the model unchanged and its predictions "invariant";
2. Projections reduce the dimension of the model;
3. Push-outs extend the model by adding factors.

These mathematical operations are versatile tools in the hand of the quantitative manager to reformulate, simplify or extend a model.

Fourth, we describe the uses of factor models. There are quite a few:
1. Forecast and Decompose volatility, so that we can separate wanted vs unwanted risk;
2. Be a fundamental input to Portfolio Construction;
3. Understand performance and separate skill from luck;
4. Serve as a foundation for alpha research.
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-113.jpg?height=673&width=1286&top_left_y=455&top_left_x=371)

Figure 4.1: A typical loadings matrix, partitioned into different blocks. The style loadings comprise an "intercept" factor (sometimes termed "country" factor). The loadings are all ones, and the intercept factor contribution to total returns is the same for all assets. The other style loadings often are standardized. The country and industry loadings take values equal to 1 if the asset belongs to the country or industry.

\subsection*{4.2. Interpretations of Factor Models}

Before we start interpreting, let us make factor model more concrete with an example. In Figure 4.1 you see a "typical" loading matrix used in a risk model. A few columns contain style loadings ${ }^{3}$. Other columns consisting of dummy variables ${ }^{4}$ indicating whether the stock belongs to a particular industry. There may be a column for "energy explorers and producers", a column for "biotechnology company", and so on. A stock will have a "1" loading if it belongs to the industry, " 0 ", otherwise. Finally there are columns consisting of dummy variables denoting country classification, similarly to industry. When the factor return for a country or an industry is high, it will move all the stocks in the industry. The factor structure captures comovement among stocks with certain obvious commonalities, as well as less obvious ones.

Even though Equation (4.1) is older than modern Statistics (having really originated in the unpublished work of Gauss), it is surprisingly rich in meaning, and possibly even richer when used in financial applications. First, let's review some interpretations of the equation.

\footnotetext{
${ }^{3}$ The use of the term "style" will be clear later, when we associate to loadings investment styles.
${ }^{4} \mathrm{~A}$ dummy variable is a variable taking binary values (e.g., 0 or 1 ) or more generally values in a finite set. We will only use binary variables.
}

\subsection*{4.2.1. Graphical Model}

The first one is as a graphical model ${ }^{5}$. Since $\mathbf{r}-\boldsymbol{\alpha}=\mathbf{B f}$, for each asset $i$ this equation holds:
$$
E\left(r_{i}-\alpha_{i} \mid f\right)=\sum_{j}[\mathbf{B}]_{i, j} f_{j}
$$

Each of the many asset returns is dependent on all, or some of, the few factor returns. In a typical regional risk model (say, America, Asia, or Europe) we have up to 10,000 assets and up to 100 factors. In Figure 4.2 we show the relationship visually. A few factors (in green) determine the expected asset returns in excess of $\boldsymbol{\alpha}$ (in yellow), through the link provided by loadings $\mathbf{B}$ (in grey). When the matrix $\mathbf{B}$ is sparse, the corresponding graph is sparse.

\footnotetext{
${ }^{5}$ Graphical models are covered in monographs [Lauritzen, 1996], books on Machine Learning [Bishop, 2006, Murphy, 2012], and survey papers [Models, 2004].
}
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-115.jpg?height=464&width=985&top_left_y=489&top_left_x=548)

Figure 4.2: Factor models as graphical models.

\subsection*{4.2.2. Superposition of Effects}

The second interpretation is as an overlap of influences on asset returns. A model for the cross-section of returns, i.e., the vector of returns at a given point in time. Let $[\mathbf{B}]_{, j}$ be the $j$ th column of the matrix $\mathbf{B}$. We rewrite $E(\mathbf{r}-\boldsymbol{\alpha})=\mathbf{B f}$ as
$$
E(\mathbf{r}-\boldsymbol{\alpha} \mid \mathbf{f})=\sum_{j}[\mathbf{B}]_{\cdot, j} f_{j}
$$

The vector of expected excess return is the superposition of a small number of vectors (the loadings $[\mathbf{B}]_{., j}$ for a specific factor), weighted by the factor return. This makes it clear that the factor component of the cross-section lives in a low-dimensional space: the column subspace of $\mathbf{B}$. This is shown in Figure 4.3.
[WILEY EDITORS: place Figure 4.3 around here.]
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-116.jpg?height=473&width=999&top_left_y=468&top_left_x=517)

Figure 4.3: A factor model as the superposition of weighted factor loadings.

\subsection*{4.2.3. Single-Asset Product}

The last interpretation applies to single assets. The expected return of an asset given the factor returns is equal to the scalar product of the asset loadings and the vector of factor returns.
$$
E\left(r_{i}-\alpha_{i} \mid \mathbf{f}\right)=\left\langle[\mathbf{B}]_{i, \cdot}, \mathbf{f}\right\rangle
$$

While this formula is rarely used at the asset level, it does show up all the time when we apply it to portfolios. Consider a portfolio $\mathbf{w} \in \mathbb{R}^{n}$, where $w_{i}$ is the Net Market Value ${ }^{6}$ invested in asset $i$; for stocks, this is the stock price times the number of shares held long or short. The expected PnL of the portfolio is
$$
\begin{aligned}
E\left(\mathbf{w}^{\top} \mathbf{r} \mid \mathbf{f}\right) & =E\left(\sum_{i} w_{i} r_{i} \mid \mathbf{f}\right) \\
& =\sum_{i}\left[\alpha_{i}+\left\langle[\mathbf{B}]_{i, \cdot}, \mathbf{f}\right\rangle\right] w_{i}
\end{aligned}
$$

\footnotetext{
${ }^{6}$ Net Market Value (NMV) is the amount invested in the security in the reference currency (numeraire).
}
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-117.jpg?height=621&width=766&top_left_y=508&top_left_x=644)

Figure 4.4: Factor models as scalar products of per-stock loadings and factor returns.

We will have an interpretation of the term $\left\langle[\mathbf{B}]_{i,}, \mathbf{f}\right\rangle w_{i}$ in Section 4.5.1. This is the PnL attributable to factor returns. We explain this factor PnL of a portfolio in terms of a scalar product; and within the scalar product identify the largest contributor, the degree of dispersion of PnL among the factors, and so on. This is the jumping-off point for performance attribution, which we will cover extensively later in the book.

\subsection*{4.3. Alpha Spanned and Alpha Orthogonal}

Consider the factor equation
$$
\mathbf{r}_{t}=\boldsymbol{\alpha}+\mathbf{B f}_{t}+\epsilon_{t}
$$
where $\mathbf{f}_{t}$ are iid with finite mean and variance, and $\boldsymbol{\epsilon}_{t}$ are iid (independent on $\mathbf{f}_{t}$ ) with zero unconditional mean and finite variance. Decompose $\boldsymbol{\alpha}$ as the sum of its projection on the column subspace of $\mathbf{B}$ (i.e., the image of the operator $\mathbf{B}$ ) and the orthogonal complement: $\boldsymbol{\alpha}=\mathbf{B} \boldsymbol{\lambda}+\boldsymbol{\alpha}_{\perp}$. By construction, the equality $\mathbf{B}^{\top} \boldsymbol{\alpha}_{\perp}=0$ holds. So we have $\mathbf{r}_{t}=\boldsymbol{\alpha}_{\perp}+\mathbf{B}\left(\boldsymbol{\lambda}+\mathbf{f}_{t}\right)+\boldsymbol{\epsilon}_{t}$. In this relationship, you can see that there is an indeterminacy in the factor model. It is useful to rewrite the model as
$$
\mathbf{r}_{t}=\boldsymbol{\alpha}_{\perp}+\mathbf{B}\left[\boldsymbol{\lambda}+E\left(\mathbf{f}_{t}\right)\right]+\mathbf{B}\left[\mathbf{f}_{t}-E\left(\mathbf{f}_{t}\right)\right]+\boldsymbol{\epsilon}_{t}
$$
where $\boldsymbol{\alpha}_{\|}:=\mathbf{B}\left[\boldsymbol{\lambda}+E\left(\mathbf{f}_{t}\right)\right]$. The "alpha" spanned by the columns of $\mathbf{B}$ is indistinguishable from the expected returns of the factors. However, the term $\boldsymbol{\alpha}_{\|}$is independent of how you split the contribution of the $\boldsymbol{\lambda}$ and $E\left(\mathbf{f}_{t}\right)$ terms. In the remainder of the book, we set $\boldsymbol{\lambda}=0$ and set $\boldsymbol{\mu}_{f}:=E\left[\mathbf{f}_{t}\right]$. Now, for a little prestige: if you choose a portfolio proportional to the alpha vector
$$
\mathrm{w}=\frac{\boldsymbol{\alpha}_{\perp}}{\left\|\boldsymbol{\alpha}_{\perp}\right\|}
$$
its payoff is
$$
\begin{aligned}
\mathbf{w}^{\top} \mathbf{r}_{t} & =\frac{1}{\left\|\boldsymbol{\alpha}_{\perp}\right\|} \boldsymbol{\alpha}_{\perp}^{\top} \mathbf{r}_{t} \\
& =\left\|\boldsymbol{\alpha}_{\perp}\right\|+\frac{\boldsymbol{\alpha}_{\perp}^{\top} \boldsymbol{\epsilon}_{t}}{\left\|\boldsymbol{\alpha}_{\perp}\right\|}
\end{aligned}
$$

The expected return and variance of this portfolio are
$$
\begin{gathered}
E\left(\mathbf{w}^{\top} \mathbf{r}_{t}\right)=\left\|\boldsymbol{\alpha}_{\perp}\right\| \\
\operatorname{var}\left(\mathbf{w}^{\top} \mathbf{r}_{t}\right)=\frac{\boldsymbol{\alpha}_{\perp}^{\top} \boldsymbol{\Omega}_{\epsilon} \boldsymbol{\alpha}_{\perp}}{\left\|\boldsymbol{\alpha}_{\perp}\right\|^{2}}
\end{gathered}
$$

There is an upper bound for the variance, given by the operator norm:
$$
\frac{\boldsymbol{\alpha}_{\perp}^{\top} \boldsymbol{\Omega}_{\epsilon} \boldsymbol{\alpha}_{\perp}}{\left\|\boldsymbol{\alpha}_{\perp}\right\|^{2}} \leq\left\|\boldsymbol{\Omega}_{\epsilon}\right\|
$$

So that
$$
\begin{aligned}
\mathrm{SR} & =\frac{E\left(\mathbf{w}^{\top} \mathbf{r}_{t}\right)}{\sqrt{\operatorname{var}\left(\mathbf{w}^{\top} \mathbf{r}_{t}\right)}} \\
& \geq \frac{\left\|\boldsymbol{\alpha}_{\perp}\right\|}{\sqrt{\left\|\boldsymbol{\Omega}_{\boldsymbol{\epsilon}}\right\|}}
\end{aligned}
$$

In the case of a diagonal matrix, $\left\|\boldsymbol{\Omega}_{\epsilon}\right\|$ is the largest element on the diagonal. Assume that it has an upper bound ${ }^{7}$. This has interesting implications. Consider the case, for example, where the average absolute orthogonal return per asset is positive:
$$
\frac{1}{n} \sum_{i}\left|\left[\boldsymbol{\alpha}_{\perp}\right]_{i}\right|=: \mu>0, \text { or, equivalently, }\left\|\boldsymbol{\alpha}_{\perp}\right\|_{1}=n \mu
$$

Now use the simple inequality between 1-norm and Euclidean norm: $\|\mathbf{x}\|_{1} \leq$ $\sqrt{n}\|\mathbf{x}\|_{2}$. Hence $\left\|\boldsymbol{\alpha}_{\perp}\right\|_{2} \geq \sqrt{n} \mu$. Apply this to the Sharpe Ratio of the alpha

\footnotetext{
${ }^{7}$ For example, for US equities, the maximum annualized idiosyncratic volatility is approximately $40 \%$.
}
orthogonal strategy, and we have a lower bound on the Sharpe Ratio:
$$
\begin{equation*}
\mathrm{SR} \geq \frac{\left\|\boldsymbol{\alpha}_{\perp}\right\|}{\sqrt{\left\|\boldsymbol{\Omega}_{\epsilon}\right\|}} \geq \sqrt{n} \frac{\mu}{\sqrt{\left\|\boldsymbol{\Omega}_{\epsilon}\right\|}} \tag{4.3}
\end{equation*}
$$

Let's summarize the assumptions we made so far, besides the fact that the factor model is correct. If we assume that:
1. the largest idiosyncratic variance ${ }^{8}$ is uniformly bounded in the number of assets.
2. the average absolute value of the coordinate of $\boldsymbol{\alpha}_{\perp}$ is bounded below by $\mu$.
then we have a lower bound on the Sharpe Strategy of a portfolio. And if these bounds are uniform for increasing values ${ }^{9}$ of $n$, then we have a sequence of Sharpe Ratios going to infinity! This is highly unlikely in real life, so that at least one of the assumptions we made - factor model, bound on idiosyncratic variances, bound on orthogonal expected returns - is likely incorrect. Under the assumption that the factor model is correct, it appears that the assumption of finite idiosyncratic variances holds in practice. This leaves us with the fact that the idiosyncratic orthogonal expected returns are vanishing in $n$. Let us summarize this result in concrete terms:
- If a linear model is a good approximation of returns, then alpha is either "spanned" or "orthogonal".
- "Alpha orthogonal" is extremely valuable: if you have positive expected alpha at the asset level, then your Sharpe increases at the rate $\sqrt{n}$, a typical rate that arises when you can diversify risk without giving up on returns.
- There are excess returns, but they are more likely to come from "alpha spanned". This alpha as we will see in the next chapter, comes with risk that does not diversify away with a large number of assets.

\footnotetext{
${ }^{8} \mathrm{Or}$, more generally, the largest eigenvalue of the sparse matrix $\boldsymbol{\Omega}_{\epsilon}$.
${ }^{9}$ Economists reason in terms of "increasing" economies as a function of the tradable assets. I believe there is not much to gain from this level of abstraction, so I keep the exposition and the inequalities in finite dimensions.
}

\subsection*{4.4. Transformations}

\subsection*{4.4.1. Rotations}

A factor model is not uniquely identified. Let $\mathbf{C}$ be an $m \times m$ invertible matrix, and define
$$
\begin{aligned}
\tilde{\mathbf{B}} & =\mathbf{B C}^{-1} \\
\tilde{\mathbf{f}} & =\mathbf{C f}
\end{aligned}
$$
the columns of $\tilde{\mathbf{B}}$ span the same subspace as the columns of $\mathbf{B}$. The model
$$
\mathbf{r}=\boldsymbol{\alpha}+\tilde{\mathbf{B}} \tilde{\mathbf{f}}+\boldsymbol{\epsilon}
$$
has the same returns as the original model. This is usually termed rotational indeterminacy of the factor model.

The covariance matrix of the transformed factors is $\tilde{\boldsymbol{\Omega}}_{\mathrm{f}}=\mathbf{C} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{C}^{\boldsymbol{\top}}$. There will be several applications of rotational indeterminacy in the book. Rotations enable us to provide final users with different views of the same model. We explore the impact of factor transformations on factor returns for three instructive examples: identity factor covariance, orthonormal loadings and z-scored
loadings.
- Identity factor covariance matrix. Sometimes, users of a model would like to see uncorrelated factor returns with unit variances. Under this perspective, exposures of the portfolio can be interpreted directly as factor volatilities, and factor risk of a portfolio is the sum of the squared exposures, without covariance terms.

This risk model perspective can be obtained by taking the Singular Value Decomposition (SVD) ${ }^{10}$ of $\boldsymbol{\Omega}_{\mathbf{f}}=\mathbf{U S U}^{\boldsymbol{\top}}$ and then setting $\mathbf{C}:=\mathbf{S}^{-1 / 2} \mathbf{U}^{\top}$. It follows that
$$
\tilde{\Omega}_{\mathbf{f}}=\mathbf{C} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{C}^{\boldsymbol{\top}}=\mathbf{S}^{-1 / 2} \mathbf{U}^{\top} \mathbf{U S U}^{\top} \mathbf{U} \mathbf{S}^{-1 / 2}=\mathbf{I}
$$
- Orthonormal loadings. We can also choose a rotation so that the loadings are orthonormal $\tilde{\mathbf{B}}^{\top} \tilde{\mathbf{B}}=\mathbf{I}_{m}$. This means that each column of $\tilde{\mathbf{B}}$ has unit norm (but not unit variance, since the column may have non-zero mean), and is orthogonal to the other. In this case the transformation is $\mathbf{C}^{-1}:=\mathbf{V} \mathbf{S}^{-1}$, where $\mathbf{V}$ and $\mathbf{S}$ come from the SVD of $\mathbf{B}=\mathbf{U S V}^{\top}$. We get $\tilde{\mathbf{B}}=\mathbf{U}$.
- Z-scored loadings. The z-scoring of factor loadings is a common procedure. It consists of a linear rescaling of the loadings of one or more factors, so that the new loadings have zero mean and unit variance (since they are zero-mean, they have also unit squared norm). The benefit of the transformation is that it makes the loadings easier to interpret. Is the stock more exposed than the average to the factor, and by how much? What is the average portfolio exposure to the factor on a standardized basis? Is such a linear transforma-

\footnotetext{
${ }^{10}$ See Appendix, Section 4.7.4, for a introduction to the SVD and the square root of a matrix, which we will be using throughout the book.
}

\subsubsection*{FAQ 4.2: Is z-scoring factor loadings changing a factor model?}
Z-scoring the loadings of a model will result in a model that makes the same predictions as the original model with raw loadings only if the unit vector $(1,1, \ldots, 1)$ can be expressed as a linear combination of the loadings vectors of the original model. A special case is the one where the "country" factor (or "intercept" factor) is among the original factors. If that is not the case, z -scoring factors will result in a model that produces different risk forecasts and performance attributions.
tion resulting in an equivalent factor model? It is possible to multiply the loadings of factor $i$ by constant $\kappa_{i}$ : just consider $\mathbf{C}:=\operatorname{diag}\left(\kappa_{1}^{-1}, \ldots, \kappa_{m}^{-1}\right)$, which is always invertible. However, in general it is not possible to center the loadings (You can try to find a counterexample in Exercise 4.4). However, assume that the unit vector is in the subspace spanned by the loadings columns, i.e., there is a vector $\mathbf{a} \in \mathbb{R}^{m}$ such that ${ }^{11} \mathbf{e}=\mathbf{B a}$. In this case, the centering is possible. If we want to add constants $\nu_{i}$ to the loadings, then $\tilde{\mathbf{B}}=\mathbf{B}+\mathbf{e e}^{\boldsymbol{\top}} \operatorname{diag}(\boldsymbol{\nu})=\mathbf{B}\left(\mathbf{I}_{m}+\mathbf{a a}^{\boldsymbol{\top}} \mathbf{B}^{\boldsymbol{\top}} \operatorname{diag}(\boldsymbol{\nu})\right)$, hence our transformation is
$$
\begin{equation*}
\mathbf{C}^{-1}=\mathbf{I}_{m}+\mathbf{a a}^{\top} \mathbf{B}^{\top} \operatorname{diag}(\boldsymbol{\nu}) \tag{4.4}
\end{equation*}
$$

This assumption is verified in two common cases. The first one is the use of a "market" (also called a "country") factor, to which all assets have unit exposure. The loadings vector is then $\mathbf{e}$ by construction. The second case is when there are country or industry loadings, such that for each asset the sum of the loadings across industries is exactly one. In this case $\mathbf{e}=\mathbf{B a}$ for a vector a that has ones in positions corresponding to the industry factors, and zero otherwise.

\footnotetext{
${ }^{11}$ We use the notation $\mathbf{e}:=(1,1, \ldots, 1)^{\top}$.
}


\subsection*{4.4.2. Projections}

Occasionally, we may want to use a risk model with fewer factors compared to the original one. At first glance, this operation may seem unjustified. If we trust that our risk model is the most accurate, why would we want to replace it with a different one? The reasons are many. For example, it may be the case that in practice the loadings of one or more factors are changing so fast as to make portfolio management and hedging difficult. Another reason is that we are using a vendor-provided model, and that we believe that the model is not perfect, i.e., some factors should be pruned from the model. A third reason may be that we want to provide the end user with a "simplified" risk model that is as accurate as possible, while retaining the full model for other uses. For these reasons and more, we need to find a different risk model that is close, in some sense, to the original one.

We have a model $\mathbf{r}=\boldsymbol{\alpha}+\mathbf{B f}+\boldsymbol{\epsilon}$ and associated covariance matrix $\boldsymbol{\Omega}_{\mathbf{r}}=$ $\mathbf{B} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{B}^{\top}+\boldsymbol{\Omega}_{\boldsymbol{\epsilon}}$, but we want to employ a different loadings matrix $\mathbf{A}$, in which the range of $\mathbf{A}$ is contained in the range of $\mathbf{B}$. If we model returns as $\mathbf{r}=\boldsymbol{\alpha}+\mathbf{A g}+\boldsymbol{\eta}$, the covariance matrix would be $\boldsymbol{\Omega}_{\mathbf{r}}=\mathbf{A} \boldsymbol{\Omega}_{\mathbf{g}} \mathbf{A}^{\boldsymbol{\top}}+\boldsymbol{\Omega}_{\boldsymbol{\eta}}$. What is the value $\mathbf{g}$ resulting in the best approximation to our original model? Let the distance between the original factor returns $\mathbf{f}$ and the approximate factor returns $\mathbf{g}$ be $\|\mathbf{B f}-\mathbf{A g}\|^{2}$. The distance-minimizing approximate factor returns are $\mathbf{g}=\mathbf{H f}$, where $\mathbf{H}:=\left(\mathbf{A}^{\top} \mathbf{A}\right)^{-1} \mathbf{A}^{\top} \mathbf{B}$. The corresponding value of $\boldsymbol{\Omega}_{g}$ is $\boldsymbol{\Omega}_{\mathrm{g}}=\mathbf{H} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{H}^{\top}$.

We call the operator a $\mathbf{H}$ projections because, like geometric projections,

\subsubsection*{FAQ 4.3: Which projections?}

What types of projections are useful in practice?
1. The columns of the matrix $\mathbf{A}$ are a subset of the columns of $\mathbf{B}$. Thus, we are attributing the maximum risk and performance to a nested factor model derived from the original one. This is by far the main application of projections.
2. The subspace spanned by the columns of $\mathbf{A}$ is not contained in that spanned by the columns of $\mathbf{B}$. In this case, the application is the qualitative ability of the second model to describe the factor risk predictions of the first model.
they are idempotent. An idempotent linear operator $\boldsymbol{\Pi}$ is such that $\boldsymbol{\Pi}^{2} \mathbf{x}=\boldsymbol{\Pi} \mathbf{x}$. The geometric intuition is that, once you have projected a vector on a plane, projecting the resulting vector again on the same plane does not result in another vector, since the input vector is already on the plane.

\subsection*{4.4.3. Push-Outs}

In the previous two sections we introduced a transformation that preserves the number of factors and a transformation that reduces it. The last section focuses on a transformation that increases the number of factors. We therefore lift the loadings matrix into a new one, whose column space contains the column space of the old one. Why could this be of interest? A possible scenario that occurs in practice is that our factor model may have been developed on historical data that are not representative of the current regime. As a result, the idiosyncratic returns show some structure, in the sense that they themselves are amenable
to be formulated as a different factor model:
$$
\epsilon=\mathbf{A g}+\eta
$$
with $\mathbf{A} \in \mathbb{R}^{n \times p}, \mathbf{g}$ a random variable (rv) taking values in $\mathbb{R}^{p}$ and $\boldsymbol{\eta}$ a rv taking values in $\mathbb{R}^{n}$. The new model becomes
$$
\begin{equation*}
\mathbf{r}=\boldsymbol{\alpha}_{\perp}+\mathbf{B f}+\mathbf{A g}+\boldsymbol{\eta} \tag{4.5}
\end{equation*}
$$

With $\boldsymbol{\eta}$ uncorrelated from $\mathbf{f}, \mathbf{g}$. In the specification of the model (4.5), we require that $\mathbf{A}^{\top} \mathbf{B}=0$. If not, the factor returns of the original model would have to be modified. To see this, assume that $\mathbf{B}^{\top} \mathbf{A} \neq 0$. Then we can decompose the columns of $\mathbf{A}$ into the sum of parallel and orthogonal components. In matrix terms, $\mathbf{A}=\mathbf{B C}+\mathbf{H}$, for some $\mathbf{C} \in \mathbb{R}^{m \times p}$. It follows that the model $\mathbf{r}=\mathbf{B f}+\mathbf{A g}+\boldsymbol{\eta}$ can be written as $\mathbf{r}=\mathbf{B}(\mathbf{f}+\mathbf{C g})+\mathbf{H g}+\boldsymbol{\eta}$.
$$
\begin{array}{rr}
\boldsymbol{\epsilon}^{\top} \mathbf{B f}=0 & \text { (original model orthogonality condition) } \\
\eta^{\top} \mathbf{A g}=0 & \text { (residual model orthogonality condition) } \\
\boldsymbol{\eta}^{\top}(\mathbf{B f}+\mathbf{A g})=0 & \text { (final model orthogonality condition) }
\end{array}
$$

From the second and the third equalities it follows that $\boldsymbol{\eta}^{\top} \mathbf{B f}=0$; the first equality can be rewritten as $0=\left(\mathbf{g}^{\top} \mathbf{A}^{\top}+\boldsymbol{\eta}^{\top}\right) \mathbf{B f}=\mathbf{g}^{\top} \mathbf{A}^{\top} \mathbf{B f}$ for all realization of $\mathbf{f}, \mathbf{g}$, hence $\mathbf{A}^{\top} \mathbf{B}=0$. The example above assumed that the idiosyncratic returns of each asset have the same volatility (homoskedastic volatilities). In Chapter 9 we will see how to augment a risk model in a Characteristic-model framework when idiosyncratic returns are heteroskedastic.

Exercise 4.1(Excess Returns and Factor Models): In the academic
literature the standard factor model (4.1) models the excess returns, defined as $\left(r_{t}\right)_{i}-r_{f}$, as per Section 2.1.2 On the other side, practitioners think in terms of returns, not excess returns.
- When in portfolio management is it incorrect to reason in terms of excess returns? When is it not?
- Show that a model of excess returns could be recasts as a model of returns by adding a factor.
- Can you extend the modeling to incorporate sensitivities to interest rates?

\subsection*{4.5. Applications}

\subsection*{4.5.1. Performance Attribution}

What is the PnL of a portfolio in interval $[t-1, t]$ ?
$$
\begin{aligned}
\left(\text { portfolio PnL } L_{t}\right) & =\mathbf{w}_{t}^{\top} \mathbf{r}_{t} \\
& =\mathbf{w}_{t}^{\top} \mathbf{B f}_{t}+\mathbf{w}_{t}^{\top}\left(\boldsymbol{\alpha}_{\perp}+\boldsymbol{\epsilon}_{t}\right) \\
& =\mathbf{b}_{t}^{\top} \mathbf{f}_{t}+\mathbf{w}_{t}^{\top}\left(\boldsymbol{\alpha}_{\perp}+\boldsymbol{\epsilon}_{t}\right) \quad\left(\mathbf{b}_{t}:=\mathbf{B}^{\top} \mathbf{w}_{t}\right)
\end{aligned}
$$

The vector $\mathbf{b}_{t} \in \mathbb{R}^{m}$ are the factor exposures of the portfolio at time $t$. The term $\left[\mathbf{b}_{t}\right]_{i}$ is the sum of the characteristics of factor $i$ of each stock, weighted by the portfolio holdings; keep in mind that the characteristics and the weights can both be negative. The term $\mathbf{b}_{t}^{\top} \mathbf{f}_{t}$ is the factor PnL in time interval $t$, while the term $\mathbf{w}_{t}^{\top}\left(\boldsymbol{\alpha}_{\perp}+\boldsymbol{\epsilon}_{t}\right)$ is the idiosyncratic PnL. Summing up over a time interval
$[1, \ldots, T]$, the PnL of a strategy is
$$
\operatorname{PnL}=(\text { Factor PnL })+(\text { Idiosyncratic PnL })
$$

We can also distribute the sum differently:
$$
\begin{aligned}
\operatorname{PnL} & =\sum_{t=1}^{T}\left(\text { Factor PnL } L_{t}\right)+\left(\text { Idiosyncratic PnL } L_{t}\right) \\
& =\sum_{t=1}^{T} \sum_{j=1}^{m}\left[\mathbf{b}_{t}\right]_{j}\left[\mathbf{f}_{t}\right]_{j}+\sum_{t=1}^{T} \sum_{i=1}^{n}\left[\mathbf{w}_{t}\right]_{i}\left[\boldsymbol{\alpha}_{\perp}+\boldsymbol{\epsilon}_{t}\right]_{i} \\
& =\sum_{j=1}^{m}(\text { Factor } j \text { PnL })+\sum_{i=1}^{n}(\text { Stock } i \text { Idiosyncratic PnL })
\end{aligned}
$$

And then, of course, one can partition factors and stocks in groups, to highlight, for example, the performance arising from style factors, from industry factors, or from a specific group of stocks.

\subsection*{4.5.2. Risk Management: Forecast and Decomposition}

If we have a covariance matrix (not specifically from a factor model), the variance of a portfolio $\mathbf{w}$ is easy to compute: $\operatorname{var}\left(\mathbf{r}^{\top} \mathbf{w}\right)=\sum_{i, j} \operatorname{cov}\left(r_{i} w_{i}, r_{j} w_{j}\right)=$ $\sum_{i, j} w_{i} \operatorname{cov}\left(r_{i}, r_{j}\right) w_{j}=\mathbf{w}^{\boldsymbol{\top}} \boldsymbol{\Omega}_{\mathbf{r}} \mathbf{w}$. We can apply the formula to a covariance matrix associated with a factor model:
$$
\begin{aligned}
\operatorname{var}\left(\mathbf{r}^{\top} \mathbf{w}\right) & =\mathbf{w}^{\top}\left(\mathbf{B} \boldsymbol{\Omega}_{\mathbf{f}} \mathbf{B}^{\top}+\boldsymbol{\Omega}_{\epsilon}\right) \mathbf{w} \\
& =\mathbf{b}^{\top} \boldsymbol{\Omega}_{\mathbf{f}} \mathbf{b}+\mathbf{w}^{\top} \boldsymbol{\Omega}_{\epsilon} \mathbf{w}
\end{aligned}
$$

The formula has two applications. The first one is an estimate of a portfolio's ex ante volatility at any point in time. This is an important piece of information for risk managers, since they monitor volatility and allocate risk based on this measure. The second application is the decomposition of variance in factor and idiosyncratic components. Like in the attribution case, the formula is a jumping-off point. For example, a commonly quoted statistic for a strategy is the percentage of idiosyncratic variance, defined as $100 \times$ (dollar idiosyncratic variance)/(total variance); the percentage of idiosyncratic variance and factor variance sum to 100 , of course. The factor variance can be decomposed further by making factor partitions. The most detailed one has each factor being a singleton, but very common choices are (style group)/(industry group)/(country group), or (subsectors group)/(style group)/(country group). This measure, and the associated sensitivities, are commonly used to monitor strategies. Every partition, either of factors or of assets, induces a covariance matrix $\boldsymbol{\Omega} \in \mathbb{R}^{p \times p}$ where $\boldsymbol{\Omega}_{i, j}$ is the covariance between partition group $i$ and $j$. For example, say that a portfolio has factor exposure b, and we partition the factors into groups $1, \ldots, p$, with group $i$ containing the subset of elements $\mathcal{S}(i)$. Define $\mathbf{b}_{\mathcal{S}(i)}$ as a vector of factor exposures where all the terms not in $\mathcal{S}(i)$ are set equal to zero. Define $\boldsymbol{\Omega}$ as the covariance matrix of the sets' PnLs:
$$
\boldsymbol{\Omega}=\left[\begin{array}{ccc}
\mathbf{b}_{\mathcal{S}(1)}^{\top} \boldsymbol{\Omega}_{1,1} \mathbf{b}_{\mathcal{S}(1)} & \ldots & \mathbf{b}_{\mathcal{S}(1)}^{\top} \boldsymbol{\Omega}_{1, p} \mathbf{b}_{\mathcal{S}(p)} \\
\ldots & \ldots & \ldots \\
\mathbf{b}_{\mathcal{S}(p)}^{\top} \boldsymbol{\Omega}_{p, 1} \mathbf{b}_{\mathcal{S}(1)} & \ldots & \mathbf{b}_{\mathcal{S}(p)}^{\top} \boldsymbol{\Omega}_{p, p} \mathbf{b}_{\mathcal{S}(p)}
\end{array}\right]
$$

Then the total factor variance is $v_{\mathrm{TOT}}^{2}:=\mathbf{e}^{\boldsymbol{\top}} \boldsymbol{\Omega} \mathbf{e}$, where $\mathbf{e}:=(1 \ldots 1)^{\top}$. The $i$ th group's variance is $v_{i}^{2}:=\mathbf{b}_{\mathcal{S}(i)}^{\top} \boldsymbol{\Omega}_{i, i} \mathbf{b}_{\mathcal{S}(i)}$.
- Fraction of total variance for group $i$ is
$$
\begin{aligned}
p_{i}: & =\frac{(\text { variance of group } i+\text { half of covariance contributions) }}{\text { (portfolio variance) }} \\
& =\frac{\sum_{j} \boldsymbol{\Omega}_{i, j}}{v_{\mathrm{TOT}}^{2}} \\
& =\frac{\operatorname{cov}\left(\mathbf{r}_{i}, \mathbf{r}_{\mathrm{TOT}}\right)}{v_{\mathrm{TOT}}^{2}} \\
& =\beta_{i} \quad \text { (beta of ith factor group's PnL to total PnL) }
\end{aligned}
$$

So that $\sum_{i} p_{i}=1$. The percentage of variance of a group $\mathcal{S}(i)$ (again, this includes single factors and single assets-perhaps the most commonly used partition!) is simply the beta of returns of the group to the overall portfolio.
- Marginal contribution to risk (MCR) of a group $\mathcal{S}(i)$ is defined as
$$
\begin{aligned}
& m_{i}\left.:=\frac{(\text { portfolio } \$ v o l}{} \text { change when we buy } \$ 1 \mathrm{M} \text { vol of set } \mathcal{S}(i)\right) \\
& \$ 1 \mathrm{M} \\
&=\left.\frac{\partial}{\partial\left(x_{i} v_{i}\right)} \sqrt{\mathbf{x}^{\top} \boldsymbol{\Omega} \mathbf{x}}\right|_{\mathbf{x}=\mathbf{e}} \\
&=\frac{1}{v_{i} v_{\mathrm{TOT}}} \sum_{j}[\boldsymbol{\Omega}]_{i, j} \\
&=\rho_{i} \quad \text { (correlation of ith factor group's PnL to total PnL) } \\
&=\frac{v_{\mathrm{TOT}}}{v_{i}} p_{i}
\end{aligned}
$$
- Sharpe Ratio sensitivity. It is also useful to compute the sensitivity of the Sharpe Ratio to changes in volatility of a group. The Total Portfolio's Sharpe

Ratio Sensitivity with respect to volatility increase of group $i$ is given by
$$
\begin{aligned}
\frac{\partial}{\partial\left(v_{i} x_{i}\right)} \frac{E(P n L)}{\operatorname{vol}(P n L)} & =\frac{\operatorname{vol}(P n L) \times \frac{\partial E(P n L)}{\partial\left(x_{i} v_{i}\right)}-\frac{\partial \mathrm{vol}(P n L)}{\partial\left(x_{i} v_{i}\right)} \times E(P n L)}{\operatorname{var}(P n L)} \\
& =\frac{\operatorname{vol}(P n L) \times \frac{\partial E(P n L)}{\partial\left(x_{i} v_{i}\right)}-m_{i} \times \mathrm{SR}_{\mathrm{TOT}} \times \operatorname{vol}(P n L)}{\operatorname{var}(P n L)} \\
& =\frac{\mathrm{SR}_{\mathrm{TOT}}}{v_{\mathrm{TOT}}}\left(\frac{\mathrm{SR}_{i}}{\mathrm{SR}_{\mathrm{TOT}}}-m_{i}\right)
\end{aligned}
$$

The contribution to total Sharpe Ratio is positive if the Sharpe Ratio of a group exceeds a threshold, which is its marginal contribution to risk times the total Sharpe Ratio.

\subsubsection*{FAQ 4.4: Do model rotations affect risk decompositions?}
When we rotate a factor model, we transform the factor loadings and factor covariance matrix as $\tilde{\mathbf{B}}=\mathbf{B C} \mathbf{C}^{-1}$ and $\tilde{\boldsymbol{\Omega}}_{\mathrm{f}}=\mathbf{C} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{C}^{\top}$ respectively. In the rotated model, the factor exposures of portfolio $\mathbf{w}$ are $\tilde{\mathbf{b}}=\tilde{\mathbf{B}}^{\top} \mathbf{w}=$ $\left(\mathbf{C}^{\top}\right)^{-1} \mathbf{b}$, and the factor risk is $\tilde{\mathbf{b}}^{\top} \tilde{\boldsymbol{\Omega}}_{\mathrm{f}} \tilde{\mathbf{b}}=\mathbf{b}^{\top} \mathbf{C}^{-1}\left(\mathbf{C} \boldsymbol{\Omega}_{\mathrm{f}} \mathbf{C}^{\boldsymbol{\top}}\right)\left(\mathbf{C}^{\boldsymbol{\top}}\right)^{-1} \mathbf{b}=$ $\mathbf{b}^{\top} \boldsymbol{\Omega}_{\mathbf{f}} \mathbf{b}$. The total factor variance is unchanged. The total (and singleasset) idiosyncratic variance is unchanged too, as it does not depend on rotations. However, the single-factor risk variance is affected by rotations. Rather than being a drawback, this is a plus. It affords us the flexibility to attribute risk to factor groups that are more meaningful (e.g., more intuitive) than others.

\subsection*{4.5.3. Portfolio Management}

Factor models are useful for portfolio management in several ways. The first one is adjacent to risk management: volatility is the common language spoken by

FAQ 4.5: Why not use the empirical covariance matrix?
Before treating factor model estimation, we address a preliminary question. Given a time series of returns $\mathbf{r}_{t}$ with population covariance matrix $\Omega_{\mathbf{r}}$, its simplest estimator is the empirical covariance
$$
\hat{\boldsymbol{\Omega}}_{\mathbf{r}}=\frac{1}{T} \sum_{t=1}^{T} \mathbf{r}_{t} \mathbf{r}_{t}^{\top}
$$
or, if we denote $\mathbf{R} \in \mathbb{R}^{n \times T}$ the matrix of returns where $\mathbf{R} \cdot, t=\mathbf{r}_{t}$, we can write $\hat{\boldsymbol{\Omega}}_{\mathbf{r}}=T^{-1} \mathbf{R} \mathbf{R}^{\top}$. It is well known (and easy to establish) that this estimate maximizes the log likelihood for a normal multivariate distribution, is asymptotically consistent, and a Central Limit Theorem is available for it [Anderson, 1963]. Why not use this as our estimate for our covariance matrix? The reason is that, when $T \ll n$, the estimator is very inadequate for volatility estimation and portfolio optimization purposes. The covariance matrix has at most $\operatorname{rank} T$. Let $\mathbf{w}_{i}, i=1, \ldots, T-n$ a basis for the null space of $\mathbf{R}^{\top}$, i.e., $\mathbf{R}^{\top} \mathbf{w}_{i}=0$. We can interpret these vectors as portfolios. The volatility of portfolio $i$ is $\mathbf{w}_{i}^{\top} \hat{\boldsymbol{\Omega}}_{\mathbf{r}} \mathbf{w}_{i}=T^{-1}\left\|\mathbf{R}^{\top} \mathbf{w}_{i}\right\|^{2}=0$. So, a majority of independent portfolios has zero volatility. The situation is even worse in portfolio optimization. The solution of the mean-variance problem $\max _{\mathbf{w}}\left[\boldsymbol{\alpha}^{\top}-(2 \lambda)^{-1} \mathbf{w}^{\top} \hat{\boldsymbol{\Omega}}_{\mathbf{r}} \mathbf{w}\right]$ is $\mathbf{w}=\lambda \boldsymbol{\Omega}_{\mathbf{r}}^{-1} \boldsymbol{\alpha}$. In this case, if $\boldsymbol{\alpha}$ is in the null space of $\boldsymbol{\Omega}$, the portfolio is undefined. Choosing an alpha close to the null space yields an arbitrarily large portfolio, and an arbitrarily large Sharpe Ratio.
risk managers and portfolio managers, and it is oftentimes generated by a factor model. The second one is the inverse of the covariance matrix also known as precision matrix. This matrix plays a central role in portfolio optimization. As discussed in FAQ 4.5, the empirical covariance matrix is usually not invertible. The factor structure makes it possible to estimate both $\boldsymbol{\Omega}_{\mathrm{r}}$ and $\boldsymbol{\Omega}_{\mathbf{r}}^{-1}$. The third is the model of the expected asset returns as the sum of two terms: $\boldsymbol{\alpha}$ and $\mathbf{B}^{\top} E(\mathbf{f})$. These two terms give rise to two qualitatively different classes of expected returns. This makes sense intuitively, since the factor-based returns come with some variability and risk, which is itself captured by the factor covariance matrix $\boldsymbol{\Omega}_{\mathbf{f}}$. The alpha term, instead, comes with apparently no risk. How to manage these sources of returns is the concern of portfolio management. Lastly, a factor model is legible: when applied to a portfolio, it produces factor exposures, risk and performance decompositions, as discussed above. Legibility makes the job of the portfolio managers easier, since it enables them to plan (before the trade), monitor (during the trade) and understand (after the trade) their strategies.

\subsection*{4.5.4. Alpha Research}

As volatility is the lingua franca spoken by risk managers and portfolio managers, so alpha is what a signal researcher and a portfolio manager both understand. At the cost of excessive generalization, one could say that the signal researcher cares about $\boldsymbol{\alpha}$, the risk manager cares about $\mathbf{B f}$ and the portfolio manager adds trading costs and tries to combine all these terms into a profitable strategy. In reality, there is "risky" alpha worth trading contained in the term Bf. I use the language loosely for the time being, with the goal of tightening it in
coming chapters. Furthermore, sometimes these people are one and the same, although the roles in sufficiently large and complex strategies are increasingly separated. Alpha research is improved by factor models in two ways. First, Bf is important, and for a certain class of investors it is the only thing that matters ${ }^{12}$. Second, the factor-based approach helps separate the two sources of expected returns for a portfolio. One source arises from having factor exposure. These returns come with the associated risk of variable factor returns. The second source is the "true intercept" of returns; i.e. having exposure to the alpha vector.

\subsection*{4.6. Factor Models Types}

The model we use from here on is given by Equation (4.1). We have taken the model for granted. But where do the data and parameters of the model come from? In the case of factor models, the answer is especially important, because the meaning attached to the various symbols matters. Practitioners use three broad approaches to identify all the parameters in the equation:
- Characteristic model: this is the most common approach. The input data to the model are the time series $\mathbf{r}_{t}$ and $\mathbf{B}_{t}$. Factor and idiosyncratic returns are estimated from these data. We define $\mathbf{B}_{t}$ as a matrix of asset characteristics available ${ }^{13}$ at the beginning of the interval $[t-1, t]$. The intuition is that these characteristics are partially responsible for the stock return. I cover this in Chapter 6.
- Statistical Model: in this model, the only primitive is $\mathbf{r}_{t}$, and $\mathbf{B}_{t}, \mathbf{f}_{t}$ and $\boldsymbol{\epsilon}_{t}$ are all estimated. I cover these models in Chapter 7.
- Macroeconomic model: in this approach, the primitives are $\mathbf{r}_{t}$ and $\mathbf{f}_{t}$, and $\mathbf{B}_{t}$ and $\boldsymbol{\epsilon}_{t}$ are estimated. $\mathbf{f}_{t}$ usually represents a vector of macroeconomic time series.

The relevant methodological issues the modeler must address are:
1. What are the best loss functions to evaluate a model?
2. Once we have estimates (or primitive data) about factor and idiosyncratic returns, how do we estimate the covariance matrices from cross-sectional estimates?
3. What is the best approach within each framework?


\footnotetext{
${ }^{12} \mathrm{~A}$ marketing term used for this investment style is smart beta.
${ }^{13}$ Important: when using commercial data, always check the data specification to make sure that the the time index for the loadings is not off by one. Most vendors use the timestamp $t$ for data available at the end of the interval $[t-1, t]$.
}

\section*{4.7. Appendix}

\subsection*{4.7.1. Linear Regression}

Linear models are by far the most widespread class of models in Statistics. There are more books on the subject than citizens of the sovereign state of the Vatican ${ }^{14}$. In fact, one could argue there is so much material on linear models, that two humans on planet Earth may have completely different interpretations of them. In order to have some common ground, we will describe some less-well known aspects which will be needed later. Our setting is as follows. We are given a pair $(y, \mathbf{x})$, where $y$ is a random variable taking values in $\mathbb{R}$ and $\mathbf{x}$ is a random vector taking values in $\mathbb{R}^{m}$. The random variables $y$ and $\mathbf{x}$ are in general dependent: knowing the value of a realization of $\mathbf{x}$ tells us something about the values of $y$ and this makes the problem infinitely interesting. Say that we want to provide a forecast of $y$, which we denote $\hat{y}(\mathbf{x})$. One way to select such forecast is to try to minimize a loss function; we should pay a price for being wrong. One natural choice of loss is the quadratic loss: it is nonnegative; it is symmetric; it is differentiable; and it penalizes more for large errors. The problem we face is
$$
\begin{equation*}
\min _{\hat{y}} E\left[(\hat{y}(\mathbf{x})-y)^{2} \mid \mathbf{x}\right] \tag{4.6}
\end{equation*}
$$

One basic result in statistics ${ }^{15}$ and in control theory is that, if $E\left(y^{2} \mid \mathbf{x}\right)<\infty$, the function that minimizes this expectation is the conditional expectation of $y$ given $\mathbf{x}$. We introduce a new variable $\epsilon$ :
$$
\begin{equation*}
y=E(y \mid \mathbf{x})+\epsilon \tag{4.7}
\end{equation*}
$$
\footnotetext{
${ }^{14}$ Not a joke: as of October 2024, the Vatican has 764 citizens; Amazon lists over 1,400 books in the "Probability and Statistics" section with "regression" in their title or subject, the vast majority of them covering linear models.
}
\footnotetext{
${ }^{15}$ Linear regression is an inexhaustible topic. Some useful references are, in order of increasing detail, Wasserman [2004], Hastie et al. [2008], Johnson and Wichern [2007], Harrell [2015], Gelman et al. [2022], Hansen [2022].
}

It follows that $E(\epsilon \mid \mathbf{x})=E(y \mid \mathbf{x})-E(E(y \mid \mathbf{x}) \mid \mathbf{x})=0$. Then use the following chain:
$$
\begin{aligned}
E\left[(\hat{y}(\mathbf{x})-y)^{2} \mid \mathbf{x}\right] & =E\left[(\hat{y}(\mathbf{x})-E(y \mid \mathbf{x})+E(y \mid \mathbf{x})-y)^{2} \mid \mathbf{x}\right] \\
& =E\left[\epsilon^{2} \mid \mathbf{x}\right]+E\left[(\hat{y}-E(y \mid \mathbf{x}))^{2} \mid \mathbf{x}\right]-2 E[\epsilon \mid \mathbf{x}](y-E(y \mid \mathbf{x})) \\
& =E\left(\epsilon^{2} \mid \mathbf{x}\right)+E\left[\left(\hat{y}-E(y \mid \mathbf{x})^{2} \mid \mathbf{x}\right]\right. \\
& \geq E\left(\epsilon^{2} \mid \mathbf{x}\right)
\end{aligned}
$$

The equality holds only if $\hat{y}=E(y \mid \mathbf{x})$. The term $E\left[\epsilon^{2} \mid \mathbf{x}\right]$ is finite, because
$$
\begin{aligned}
E\left(\epsilon^{2} \mid \mathbf{x}\right) & \leq 2 E\left(y^{2} \mid \mathbf{x}\right)+2 E\left[E(y \mid \mathbf{x})^{2} \mid \mathbf{x}\right] \\
& \leq 2 E\left(y^{2} \mid \mathbf{x}\right)+2 E\left[E\left(y^{2} \mid \mathbf{x}\right) \mid \mathbf{x}\right] \\
& =4 E\left(y^{2} \mid \mathbf{x}\right) \quad \text { (Jensen) } \\
& <\infty
\end{aligned}
$$

In applications, we have $n$ samples $\left(y_{i}, \mathbf{x}_{i}\right)$ and we choose a functional form for $\hat{y}=g(\mathbf{x}, \boldsymbol{\theta})$ where $\boldsymbol{\theta}$ is a finite- or infinite-dimensional vector of parameters. We then minimize the empirical squared loss $n^{-1} \sum_{i}\left(y_{i}-g(\mathbf{x}, \boldsymbol{\theta})\right)^{2}$. The simplest form of $g$ is linear: $g(\mathbf{x}, \boldsymbol{\beta})=\sum_{i} \beta_{i} x_{i}$. In matrix form, Equation (4.7) becomes
$$
\begin{equation*}
\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon} \tag{4.8}
\end{equation*}
$$
where $\mathbf{y} \in \mathbb{R}^{n}, \mathbf{X} \in \mathbb{R}^{n \times m}, \boldsymbol{\beta} \in \mathbb{R}^{m}$. The integers $n$ and $m$ denote respectively the number of observations and the "features" respectively. We want to estimate the parameters $\boldsymbol{\beta}$, leading to estimates for $\mathbf{X} \boldsymbol{\beta}$. We then minimize the empirical
loss
$$
\begin{equation*}
\min _{\boldsymbol{\beta}}\|\mathbf{y}-\mathbf{X} \boldsymbol{\beta}\|^{2} \tag{4.9}
\end{equation*}
$$
which is equal to the unweighted sum of squared errors (Ordinary Least Squares, or OLS) $\left(y_{i}-\sum_{j}[\mathbf{X}]_{i, j} \beta_{j}\right)^{2}$. A different way to arrive at the same problem is to posit that the true model is Equation (4.8), and to further assume that $\boldsymbol{\epsilon} \sim N\left(0, \sigma^{2} \mathbf{I}_{n}\right)$. If we fix $\boldsymbol{\beta}$, we have $\boldsymbol{\epsilon}=\mathbf{y}-\mathbf{X} \boldsymbol{\beta}$; and since we know the distribution of $\boldsymbol{\epsilon}$, we can associate to a choice of $\boldsymbol{\beta}$ a likelihood $f(\boldsymbol{\epsilon} \boldsymbol{\beta})$. If we choose the parameter $\boldsymbol{\beta}$ to maximize the likelihood, we end up solving the same problem as Equation (4.9). The choice of maximizing the likelihood is called the Maximum Likelihood Principle ${ }^{16}$.

Finally, there is a geometrical interpretation for the regression problem. You can interpret the set $S:=\left\{\mathbf{X} \boldsymbol{\beta} \mid \boldsymbol{\beta} \in \mathbb{R}^{m}\right\}$ as a subspace of $\mathbb{R}^{n}$. The columns of $\mathbf{X}$ are a (generally non-orthonormal) basis of the subspace. We are then given a point $\mathbf{y} \in \mathbb{R}$ and find the point $\hat{\mathbf{y}} \in S$ that is closest to $\mathbf{y}$. This is the definition of a projection of $\mathbf{y}$ on $S$. The projection is a linear operator. The minimum distance ${ }^{17}$ is attained at
$$
\begin{equation*}
\hat{\boldsymbol{\beta}}=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y} \tag{4.10}
\end{equation*}
$$

\footnotetext{
${ }^{16}$ For a detailed discussion of the MLP, see Robert [2007].
${ }^{17}$ The minimum is unique if the rank of $\mathbf{X}$ is $m$, i.e., if all the columns of $\mathbf{X}$ are linearly independent. In Chapter 6 we will encounter cases of rank-deficient matrices.
}
and the estimates $\hat{\mathbf{y}}:=E(\mathbf{y} \mid \boldsymbol{\beta})$ are
$$
\begin{align*}
\hat{\mathbf{y}} & =\mathbf{X} \hat{\boldsymbol{\beta}} \\
& =\mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y} \tag{4.11}
\end{align*}
$$

The matrix $\mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top}$ is called the hat matrix or projection matrix. The estimated residuals are
$$
\hat{\epsilon}=(\mathbf{I}-\mathbf{H}) \mathbf{y}
$$

Intuitively, the optimal estimates should not change if we change the basis of the subspace. To see this rigorously, transform $\mathbf{X}$ into $\mathbf{X Q}$, where $\mathbf{Q} \in \mathbb{R}^{m \times m}$ is non-singular. The transformed set of predictors spans the same subspace as X. Then
$$
\begin{align*}
\hat{\mathbf{y}} & =\mathbf{X Q}\left((\mathbf{X Q})^{\top} \mathbf{X Q}\right)^{-1}(\mathbf{X Q})^{\top} \mathbf{y} \\
& =\mathbf{X Q}\left(\mathbf{Q}^{\top} \mathbf{X}^{\top} \mathbf{X Q}\right)^{-1} \mathbf{Q}^{\top} \mathbf{X}^{\top} \mathbf{y} \\
& =\mathbf{X Q Q} \mathbf{Q}^{-1}\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1}\left(\mathbf{Q}^{\top}\right)^{-1} \mathbf{Q}^{\top} \mathbf{X}^{\top} \mathbf{y} \\
& =\mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{y} \tag{4.12}
\end{align*}
$$
hence $\mathbf{y}$ is independent of base representation.
Another property of the estimate $\hat{\mathbf{y}}$ is that, if we iterate the estimation process on the estimate $\hat{\mathbf{y}}$, we obtain again $\hat{\mathbf{y}}$. This also has a geometric interpretation. Once a point has been projected on a hyperplane, the projection of the projection is unchanged. In algebraic terms, $\mathbf{H} \hat{\mathbf{y}}=\mathbf{H}^{2} \mathbf{y}=\mathbf{H y}=\hat{\mathbf{y}}$.

Here is another facet of linear regression tying geometric and algebraic
interpretations of linear regression. Decompose $\mathbf{X}$ using the SVD: $\mathbf{X}=\mathbf{U} \boldsymbol{\Lambda} \mathbf{V}^{\top}$. $\mathbf{U}$ is an orthonormal basis for the column subspace of $\mathbf{x}$. Then
$$
\begin{aligned}
\hat{\mathbf{y}} & =\mathbf{U} \boldsymbol{\Lambda} \mathbf{V}^{\top}\left(\mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^{\top} \mathbf{U} \boldsymbol{\Lambda} \mathbf{V}^{\top}\right)^{-1} \mathbf{V} \boldsymbol{\Lambda} \mathbf{U}^{\top} \mathbf{y} \\
& =\mathbf{U} \mathbf{U}^{\top} \mathbf{y}
\end{aligned}
$$

So $\mathbf{y}$ is projected on the column space of $\mathbf{U}$.
Replace Equation (4.8) in beta estimation formula (4.10) to obtain
$$
\begin{aligned}
\hat{\boldsymbol{\beta}} & =\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}) \\
& =\boldsymbol{\beta}+\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \boldsymbol{\epsilon}
\end{aligned}
$$

The estimate of beta is unbiased, because $E\left[\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \boldsymbol{\epsilon}\right]=0$; and the covariance matrix of $\hat{\boldsymbol{\beta}}$ is
$$
\begin{equation*}
\operatorname{var}(\hat{\boldsymbol{\beta}})=\sigma^{2}\left(\mathbf{X}^{\boldsymbol{\top}} \mathbf{X}\right)^{-1} \tag{4.13}
\end{equation*}
$$

Similarly,
$$
\operatorname{var}(\hat{\boldsymbol{y}})=\sigma^{2} \mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top}
$$

We can write these formulas using the SVD:
$$
\begin{align*}
& \operatorname{var}(\hat{\boldsymbol{\beta}})=\sigma^{2} \mathbf{V} \boldsymbol{\Lambda}^{-2} \mathbf{V}^{\top}  \tag{4.14}\\
& \operatorname{var}(\hat{\mathbf{y}})=\sigma^{2} \mathbf{U} \mathbf{U}^{\top} \tag{4.15}
\end{align*}
$$

The variance of the estimates $\operatorname{var}(\hat{\boldsymbol{\beta}})$ becomes larger as the columns of $X$ become more collinear. In our interpretation of the matrix $\mathbf{X}$, this occurs when
we include factors that overlap heavily with pre-existing ones.
The estimation formulas extend directly to the case of heteroskedastic noise. In this case we assume that $\boldsymbol{\epsilon} \sim N\left(0, \boldsymbol{\Omega}_{\boldsymbol{\epsilon}}\right)$, where $\boldsymbol{\Omega}_{\boldsymbol{\epsilon}}$ is a positive definite matrix. The estimates for $\hat{\boldsymbol{\beta}}, \hat{\mathbf{y}}$ and $\operatorname{var}(\hat{\boldsymbol{\beta}})$ can be derived directly from the previous formulas, by left-multiplying by $\boldsymbol{\Omega}_{\epsilon}^{-1 / 2}$ both sides of Equation (4.8):
$$
\boldsymbol{\Omega}_{\epsilon}^{-1 / 2} \mathbf{y}=\boldsymbol{\Omega}_{\epsilon}^{-1 / 2} \mathbf{X} \boldsymbol{\beta}+\boldsymbol{\Omega}_{\epsilon}^{-1 / 2} \boldsymbol{\epsilon}
$$

Notice that the $\boldsymbol{\Omega}_{\epsilon}^{-1 / 2} \boldsymbol{\epsilon}$ is distributed according to a standard normal (exercise), so that the noise is homoskedastic; and we apply the OLS results to obtain the Weighted Least Squares (WLS) formulas:
$$
\begin{align*}
\hat{\boldsymbol{\beta}} & =\left(\mathbf{X}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{y}  \tag{4.16}\\
\operatorname{var}(\hat{\boldsymbol{\beta}}) & =\left(\mathbf{X}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{X}\right)^{-1}  \tag{4.17}\\
\hat{\mathbf{y}} & =\mathbf{X}\left(\mathbf{X}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{y} \tag{4.18}
\end{align*}
$$

Exercise 4.2: If a matrix $\mathbf{X} \in \mathbb{R}^{n \times m}$ has near collinear columns, then there is a unit-norm vector $\mathbf{u}$ such that $\|\mathbf{X} \mathbf{u}\|^{2}=h$ for some small positive $h$.
1. Show that $\mathbf{u}^{\top}\left(\mathbf{X}^{\top} \mathbf{X}\right) \mathbf{u}=h$.
2. Let $\lambda_{i}$ be the eigenvalues of $\mathbf{X}^{\top} \mathbf{X}$. Show that $\min _{i} \lambda_{i}^{2} \leq h$.
3. From this, show that $\sum_{i} \operatorname{var}\left(\hat{\boldsymbol{\beta}}_{i}\right)^{2} \geq \max _{i} \lambda_{i}^{-2} \geq 1 / h=\|\mathbf{X u}\|^{-2}$.

\subsection*{4.7.2. Linear Regression Decomposition}

Split Equation (4.8) into two parts:
$$
\mathbf{y}=\left[\begin{array}{ll}
\mathbf{X}_{1} & \mathbf{X}_{2}
\end{array}\right]\left[\begin{array}{l}
\boldsymbol{\beta}_{1}  \tag{4.19}\\
\boldsymbol{\beta}_{2}
\end{array}\right]+\boldsymbol{\epsilon}
$$
where we have partitioned the predictors $\mathbf{X}$ into two blocks. Equation (4.10) can be rewritten by using block matrices for $\mathbf{X}^{\top} \mathbf{X}$ and $\mathbf{X}^{\top} \mathbf{y}$, and the formula for the inverse of block matrices, in order to obtain $\hat{\boldsymbol{\beta}}_{1}, \hat{\boldsymbol{\beta}}_{2}$. It can be shown that the coefficient $\hat{\boldsymbol{\beta}}_{2}$ can be estimated by a two-step process. First, regress the columns of $\mathbf{X}_{2}$ on those of $\mathbf{X}_{1}: \mathbf{X}_{2}=\mathbf{X}_{1} \boldsymbol{\gamma}+\tilde{\mathbf{X}}_{2}$, where $\left(\mathbf{X}_{2}-\mathbf{X}_{1} \boldsymbol{\gamma}\right)^{\top} \tilde{\mathbf{X}}_{2}=0$. The matrix $\tilde{\mathbf{X}}_{2}$ contains the components of the column vectors of $\mathbf{X}_{2}$ that are orthogonal to the columns of $\mathbf{X}_{1}$ (we say that $\tilde{\mathbf{X}}_{2}$ is the projection on the orthogonal complement of the subspace spanned $\mathbf{X}_{1}$ ). The subspace spanned by $\left[\mathbf{X}_{1} \mid \tilde{\mathbf{X}}_{2}\right]$ is the same as the subspace spanned by $\left[\mathbf{X}_{1} \mid \mathbf{X}_{2}\right]$ (if you do not see it, prove it). Therefore, the estimates $\hat{\mathbf{y}}$ and $\hat{\boldsymbol{\epsilon}}$ in Equation (4.19) are unchanged. Second, regress $\mathbf{y}$ on $\tilde{\mathbf{X}}_{2}$ :
$$
\mathbf{y}=\tilde{\mathbf{X}}_{2} \boldsymbol{\beta}_{3}+\boldsymbol{\eta}
$$

It can be proven (see, e.g., Hansen [2022], Ch. 2.23; or prove it yourself as an exercise) that least-squares coefficient of this regression is the same as $\hat{\boldsymbol{\beta}}_{2}$, i.e. $\hat{\boldsymbol{\beta}}_{3}=\hat{\boldsymbol{\beta}}_{2}$.

\subsection*{4.7.3. The Frisch-Waugh-Lovell Theorem}

Let us continue along the line of reasoning of the previous section, where we characterized groups of regressors, as in Equation (4.19). As we did above, we
remove from the columns of $\mathbf{X}_{2}$ the component collinear to the columns of $\mathbf{X}_{1}$ and denote the resulting matrix $\tilde{\mathbf{X}}_{2}$.

However, this transformation enables us to perform regressions in consecutive stages, where each stage solves a stand-alone linear estimation problem. This insight is formalized in the following theorem.

Theorem 4.1(Frisch-Waugh-Lovell): Denote the reference model
$$
\begin{equation*}
\mathbf{y}=\mathbf{X}_{1} \boldsymbol{\beta}_{1}+\tilde{\mathbf{X}}_{2} \boldsymbol{\beta}_{2}+\boldsymbol{\epsilon} \tag{4.20}
\end{equation*}
$$
whose estimated parameters are $\hat{\boldsymbol{\beta}}_{1}, \hat{\boldsymbol{\beta}}_{2}, \hat{\boldsymbol{\epsilon}}$.
Estimate the system in stages. The first stage model is
$$
\begin{equation*}
\mathbf{y}=\mathbf{X}_{1} \boldsymbol{\gamma}_{1}+\boldsymbol{\eta}_{1} \tag{4.21}
\end{equation*}
$$
from which we get estimates $\hat{\boldsymbol{\gamma}}_{1}, \hat{\boldsymbol{\eta}}_{1}$.
The second-stage model uses $\hat{\boldsymbol{\eta}}_{1}$ from the first stage:
$$
\begin{equation*}
\hat{\boldsymbol{\eta}}_{1}=\tilde{\mathbf{X}}_{2} \gamma_{2}+\boldsymbol{\eta}_{2} \tag{4.22}
\end{equation*}
$$
from which we get estimates $\hat{\gamma}_{2}, \hat{\boldsymbol{n}}_{2}$. The following identities hold:
$$
\begin{aligned}
\hat{\gamma}_{1} & =\hat{\boldsymbol{\beta}}_{1} \\
\hat{\boldsymbol{\gamma}}_{2} & =\hat{\boldsymbol{\beta}}_{2} \\
\hat{\boldsymbol{\eta}}_{2} & =\hat{\boldsymbol{\epsilon}}
\end{aligned}
$$

Proof. We use the hat matrix of $\mathbf{X}_{1}, \mathbf{H}=\mathbf{X}_{1}\left(\mathbf{X}_{1}^{\top} \mathbf{X}_{1}\right)^{-1} \mathbf{X}_{1}^{\top}$. The operator is a projection, i.e., $\mathbf{H}^{2}=\mathbf{H}$ and $\left(\mathbf{I}_{n}-\mathbf{H}\right)^{2}=\mathbf{I}_{n}-\mathbf{H}$. First, we characterize
the solutions for $\hat{\boldsymbol{\beta}}_{1}, \hat{\boldsymbol{\beta}}_{2}$. We use the property that $\mathbf{X}_{1}^{\top} \tilde{\mathbf{X}}_{2}=\mathbf{X}_{1}^{\top}\left(\mathbf{I}_{n}-\mathbf{H}\right) \mathbf{X}_{2}=$ $(\mathbf{X}-\mathbf{X}) \mathbf{X}_{2}=0$.
$$
\begin{aligned}
\mathbf{y} & =\left[\begin{array}{ll}
\mathbf{X}_{1} & \tilde{\mathbf{X}}_{2}
\end{array}\right]\left[\begin{array}{c}
\boldsymbol{\beta}_{1} \\
\boldsymbol{\beta}_{2}
\end{array}\right]+\boldsymbol{\epsilon} \\
{\left[\begin{array}{c}
\hat{\boldsymbol{\beta}}_{1} \\
\hat{\boldsymbol{\beta}}_{2}
\end{array}\right] } & =\left[\begin{array}{ll}
\mathbf{X}_{1}^{\top} \mathbf{X}_{1} & \mathbf{X}_{1}^{\top} \tilde{\mathbf{X}}_{2} \\
\tilde{\mathbf{X}}_{2}^{\top} \mathbf{X}_{1} & \tilde{\mathbf{X}}_{2}^{\top} \tilde{\mathbf{X}}_{2}
\end{array}\right]^{-1}\left[\begin{array}{l}
\mathbf{X}_{1}^{\top} \\
\tilde{\mathbf{X}}_{2}^{\top}
\end{array}\right] \mathbf{y} \\
\hat{\boldsymbol{\beta}}_{1} & =\left(\mathbf{X}_{1}^{\top} \mathbf{X}_{1}\right)^{-1} \mathbf{X}_{1}^{\top} \mathbf{y} \\
\hat{\boldsymbol{\beta}}_{2} & =\left(\tilde{\mathbf{X}}_{2}^{\top} \tilde{\mathbf{X}}_{2}\right)^{-1} \tilde{\mathbf{X}}_{2}^{\top} \mathbf{y}
\end{aligned}
$$

Now, let us write the outputs $\hat{\boldsymbol{\beta}}_{1}, \tilde{\mathbf{X}}_{2}$ of the first stage:
$$
\begin{aligned}
\hat{\boldsymbol{\eta}}_{1} & =\left(\mathbf{I}_{n}-\mathbf{H}\right) \mathbf{y} \\
\tilde{\mathbf{X}}_{2} & =\left(\mathbf{I}_{n}-\mathbf{H}\right) \mathbf{X}_{2}
\end{aligned}
$$

Those are used to generate $\hat{\boldsymbol{\gamma}}_{1}, \hat{\boldsymbol{\gamma}}_{2}$. We show that they are identical to $\hat{\boldsymbol{\beta}}_{1}, \hat{\boldsymbol{\beta}}_{2}$.
$$
\begin{aligned}
\hat{\boldsymbol{\gamma}}_{1} & =\left(\mathbf{X}_{1}^{\top} \mathbf{X}_{1}\right)^{-1} \mathbf{X}_{1}^{\top} \mathbf{y} \\
& =\hat{\boldsymbol{\beta}}_{1} \\
\hat{\boldsymbol{\gamma}}_{2} & =\left(\tilde{\mathbf{X}}_{2}^{\top} \tilde{\mathbf{X}}_{2}\right)^{-1} \tilde{\mathbf{X}}_{2}^{\top} \hat{\boldsymbol{\eta}}_{1} \\
& =\left(\mathbf{X}_{2}^{\top}\left(\mathbf{I}_{n}-\mathbf{H}\right)^{2} \mathbf{X}_{2}\right)^{-1} \mathbf{X}_{2}^{\top}\left(\mathbf{I}_{n}-\mathbf{H}\right) \mathbf{y} \\
& =\left(\tilde{\mathbf{X}}_{2}^{\top} \tilde{\mathbf{X}}_{2}\right)^{-1} \tilde{\mathbf{X}}_{2}^{\top} \mathbf{y} \\
& =\hat{\boldsymbol{\beta}}_{2}
\end{aligned}
$$

The equality of $\hat{\boldsymbol{\eta}}_{2}$ and $\hat{\boldsymbol{\epsilon}}$ follows from Equations (4.20, 4.21, 4.22).
We close the section with two remarks.
- In Section 4.7.2 we showed that Equation (4.22) yields the same estimate as the regression on the total returns, i.e. that the coefficient estimated using $\hat{\boldsymbol{\eta}}_{1}=\tilde{\mathbf{X}}_{2} \boldsymbol{\gamma}_{2}+\boldsymbol{\eta}_{2}$ and $\mathbf{r}=\tilde{\mathbf{X}}_{2} \boldsymbol{\beta}_{3}+\boldsymbol{\eta}$ give identical estimates: $\hat{\boldsymbol{\gamma}}_{2}=\hat{\boldsymbol{\beta}}_{3}$. This means that, after orthogonalizing the independent variable, we have the option of regressing directly on the total return. However, the estimated residuals $\hat{\boldsymbol{\eta}}_{2}$ and $\hat{\boldsymbol{\eta}}$ will be different.
- The formulas above hold for the case of identical idiosyncratic volatilities.

For the general case, the formula for $\tilde{\mathbf{X}}_{2}$ becomes
$$
\tilde{\mathbf{X}}_{2}=\left(\mathbf{I}_{n}-\mathbf{X}_{1}\left(\mathbf{X}_{1}^{\top} \boldsymbol{\Omega}_{\epsilon}^{-1} \mathbf{X}_{1}\right)^{-1} \mathbf{X}_{1} \boldsymbol{\Omega}_{\epsilon}^{-1}\right) \mathbf{X}_{2}
$$

Proving this is left as an exercise (whiten the returns by premultiplying by $\boldsymbol{\Omega}_{\epsilon}^{-1 / 2}$, use the results above, and transform back).

\section*{Procedure 4.1: Stagewise linear regression}
1. Estimate the model
$$
\begin{equation*}
\mathbf{y}=\mathbf{X} \boldsymbol{\beta}_{1}+\boldsymbol{\epsilon} \tag{4.23}
\end{equation*}
$$
to obtain estimates $\hat{\boldsymbol{\beta}}_{1}$, and $\hat{\boldsymbol{\epsilon}}$.
2. Regress the columns of $\mathbf{Z}$ on $\mathbf{X}$ and take the residuals of each regression. Define $\tilde{\mathbf{Z}}$ as a matrix whose $i$ th column is the residual vector of $[\mathbf{Z}]_{\cdot, i}$ on $\mathbf{X}$.
3. Estimate the model $\hat{\boldsymbol{\epsilon}}=\tilde{\mathbf{Z}} \boldsymbol{\beta}_{2}+\boldsymbol{\xi}$ to obtain estimates $\hat{\boldsymbol{\beta}}_{2}$ and $\hat{\boldsymbol{\xi}}$.

\subsection*{4.7.4. The Singular Value Decomposition}

The Singular Value Decomposition (SVD) is a fundamental factorization in numerical linear algebra. It powers many numerical computations, as Golub and Van Loan [2012] beautifully explain. In addition, it is extremely insightful in theoretical analysis. Much of this book relies on it. Since it is not always covered in linear algebra courses, this appendix provides a crash course on the subject. For gentler introductions, see Trefethen and Bau [1997], Horn and Johnson [2012], Strang [2019], and the aforementioned classic book by Golub and Van Loan.

We start by recalling a basic fact of algebra. We are given a square matrix A that is symmetric and positive semidefinite $\left(\mathbf{x A} \mathbf{x}^{\top} \geq 0\right.$ for all $\left.\mathbf{x}\right)$. Let $\lambda_{i}, \mathbf{v}_{i}$ be the $i$ th eigenvalue and eigenvector of $\mathbf{A}$, i.e., $\mathbf{A v}_{i}=\lambda_{i} \mathbf{v}_{i}$, where $\mathbf{v}_{i}$ are unit-norm vectors. Then, the eigenvalues are real, positive, and the eigenvectors are orthonormal, i.e., $\mathbf{v}_{i}^{\top} \mathbf{v}_{j}=\delta_{i, j}$. What can be said about generic rectangular matrices $\mathbf{A} \in \mathbb{R}^{m \times n}$ ? A possible generalization is to relax the condition that $\mathbf{v}_{i}$ appear on both sides of the eigenvalue equation. We posit an equation of the form: $\mathbf{A} \mathbf{v}_{i}=s_{i} \mathbf{u}_{i}$, with $\mathbf{v}_{i} \in \mathbb{R}^{n}$ and $\mathbf{u}_{i} \in \mathbb{R}^{m}$. However, we keep the requirement that $\mathbf{v}_{i}$ be orthonormal and similarly for $\mathbf{U}_{i}$. Let $r \leq \min \{m, n\}$ be the rank of $\mathbf{A}$. The image subspace of $\mathbf{A}$ has dimension $r$ : there are $r$ independent vectors $\mathbf{x}_{i}$ such $\mathbf{A} \mathbf{x}_{i} \neq 0$. The kernel subspace has dimension $n-r$ : there are $m-r$ independent vectors $\mathbf{y}_{i}$ such that $\mathbf{A y}_{i}=0$. We partition $\mathbf{v}_{i}$ in image and kernel vectors:
$$
\begin{array}{rlrl}
\mathbf{A} \mathbf{v}_{i} & =s_{i} \mathbf{u}_{i} & & 1 \leq i \leq r \\
\mathbf{A v}_{i} & =0 & r & <i \leq m \tag{4.25}
\end{array}
$$

We can write these equations in matrix form:
$$
\mathbf{A}\left[\begin{array}{ccc}
\mid & \mid & \mid  \tag{4.26}\\
\mathbf{v}_{1} & \ldots & \mathbf{v}_{n} \\
\mid & \mid & \mid
\end{array}\right]=\left[\begin{array}{ccc}
\mid & \mid & \mid \\
\mathbf{u}_{1} & \ldots & \mathbf{u}_{m} \\
\mid & \mid & \mid
\end{array}\right]\left[\begin{array}{cccccc}
s_{1} & 0 & \ldots & \ldots & \ldots & 0 \\
0 & s_{2} & \ldots & \ldots & \ldots & 0 \\
0 & 0 & \ldots & \ldots & \ldots & 0 \\
0 & 0 & \ldots & s_{r} & \ldots & 0 \\
0 & \ldots & \ldots & \ldots & \ldots & 0 \\
0 & \ldots & \ldots & \ldots & \ldots & 0
\end{array}\right]
$$

Here, in addition to the vectors $\mathbf{u}_{1}, \ldots, \mathbf{u}_{r}$, we have completed this orthonormal basis with $\mathbf{u}_{r+1}, \ldots, \mathbf{u}_{m}$ so that it spans $\mathbb{R}^{m}$. In compact form, Equation (4.26) can be written as $\mathbf{A V}=\mathbf{U S}$, where $\mathbf{U}, \mathbf{V}$ are orthonormal matrices, i.e., $\mathbf{U}^{\top} \mathbf{U}=\mathbf{I}_{m}$ and $\mathbf{V}^{\top} \mathbf{V}=\mathbf{I}_{n}$; and $\mathbf{S} \in \mathbb{R}^{m \times n}$ may have non-zero elements only on the main diagonal. Finally, we rewrite the equation after right-multiplying by $\mathbf{V}^{\top}$ as
$$
\begin{equation*}
\mathbf{A}=\mathbf{U S V}^{\top} \tag{4.27}
\end{equation*}
$$

We show the decomposition visually in Figure 4.5.
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-147.jpg?height=310&width=1004&top_left_y=1840&top_left_x=514)

Figure 4.5: Singular Value Decomposition, full form.

We prove Equations (4.24), (4.25) by noting that $\mathbf{A}^{\top} \mathbf{A}$ is a positive semidefinite matrix of rank $r$, so that there are $r$ pairs $\left(\mathbf{v}_{i}, \lambda_{i}\right)$ satisfying $\left(\mathbf{A}^{\top} \mathbf{A}\right) \mathbf{v}_{i}=\lambda_{i} \mathbf{v}_{i}$.

Define $s_{i}:=\sqrt{\lambda_{i}}$ and $\mathbf{u}_{i}:=\left(\mathbf{A v}_{i}\right) / s_{i}$. These satisfy Equation (4.24). We prove that the $\mathbf{U}_{i}$ are orthonormal:
$$
\begin{equation*}
\mathbf{u}_{i}^{\top} \mathbf{u}_{j}=\frac{\mathbf{v}_{i}^{\top}\left(\mathbf{A}^{\top} \mathbf{A} \mathbf{v}_{j}\right)}{s_{i} s_{j}}=\frac{s_{j}}{s_{i}} \mathbf{v}_{i} \mathbf{v}_{j}=\delta_{i, j} \tag{4.28}
\end{equation*}
$$
because $\mathbf{v}_{1}, \ldots, \mathbf{v}_{r}$ are orthonormal. Now we complete the basis in $\mathbb{R}^{n}$ by adding orthonormal vectors $\mathbf{v}_{r+1}, \ldots, \mathbf{v}_{n}$, where $\mathbf{A v}_{i}=0$. These make a basis for the nullspace of A. Correspondingly, we add orthonormal vectors $\mathbf{u}_{r+1}, \ldots, \mathbf{u}_{m}$ to complete the basis in $\mathbb{R}^{m}$. A few observations (among many) on the SVD:
1. If all the singular values are distinct, the first $r$ columns of $\mathbf{U}$ and $\mathbf{V}$ are uniquely determined. However, they are not in the case of identical singular values.
2. The power of a symmetric positive definite matrix $\mathbf{A}$, and in particular the square root, can be easily defined from the SVD. $\mathbf{A}^{a}=\left(\mathbf{U S U}^{\top}\right)^{a}=$ $\mathbf{U} \operatorname{diag}\left(s_{1}^{a}, \ldots, s_{n}^{a}\right) \mathbf{U}^{\top}$. It is easy to show that this definition meets the requirement of exponentiation. For the specific case of the square root, one can show that $\mathbf{A}^{1 / 2} \mathbf{A}^{1 / 2}=\mathbf{A}$.
3. Equation (4.27) can be rewritten as
$$
\begin{equation*}
\mathbf{A}=\sum_{i=1}^{r} s_{i} \mathbf{u}_{i} \mathbf{v}_{i}^{\top} \tag{4.29}
\end{equation*}
$$

The SVD decomposes a matrix into a sum of rank-one matrices.
4. For all $i \leq r$,
$$
\begin{align*}
& \mathbf{A}^{\top} \mathbf{A} \mathbf{v}_{i}=s_{i}^{2} \mathbf{v}_{i}  \tag{4.30}\\
& \mathbf{A} \mathbf{A}^{\top} \mathbf{u}_{i}=\mathbf{A}\left(\mathbf{A}^{\top} \mathbf{A} \mathbf{v}_{i}\right) / s_{i}=s_{i} \mathbf{A} \mathbf{v}_{i}=s_{i}^{2} \mathbf{u}_{i} \tag{4.31}
\end{align*}
$$

In other terms, $\mathbf{A}^{\top} \mathbf{A}$ and $\mathbf{A} \mathbf{A}^{\top}$ have the same eigenvalues.
5. The SVD decomposes the operations on an element in $\mathbb{R}^{n}$ into a rotation, a rescaling of the axes turning a ball into an ellipsoid, followed by another rotation. The net result is that any operator A maps a point on a ball into a point on a rotated ellipsoid. Figure 4.6 illustrates the steps of the SVD.
![](https://cdn.mathpix.com/cropped/2024_12_18_9227ea04167b5cb23644g-149.jpg?height=525&width=1329&top_left_y=964&top_left_x=367)

Figure 4.6: Singular Value Decomposition as a sequence of steps: rotation, scaling, rotation.

\subsection*{4.8. Exercises}

\section*{Exercise 4.3(Portfolio Covariances):}
1. Generalize this result. Let x be a random vector taking values in $\mathbb{R}^{n}$ with covariance matrix $\boldsymbol{\Omega}$. Let $A$ be an $m \times n$ matrix. Prove that the covariance matrix of the random vector $\mathbf{A x}$ is $\mathbf{A} \boldsymbol{\Omega} \mathbf{A}^{\top}$.
2. Say that a random vector $\mathbf{x}$ follows a multivariate normal distribution with covariance matrix $\boldsymbol{\Omega}$. Let the Singular Value Decomposition of $\boldsymbol{\Omega}$ be
$\mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^{\top}$, and define
$$
\boldsymbol{\Omega}^{1 / 2}=\mathbf{U}\left[\begin{array}{ccccc}
\lambda_{1}^{1 / 2} & 0 & \ldots & \ldots & 0 \\
0 & \lambda_{2}^{1 / 2} & 0 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots & 0 \\
0 & 0 & 0 & \ldots & \lambda_{n}^{1 / 2}
\end{array}\right] \mathbf{U}^{\top}
$$

Let $\boldsymbol{\xi}$ a Gaussian distribution with unit covariance matrix. Prove that $\boldsymbol{\Omega}^{1 / 2} \boldsymbol{\xi}$ has covariance $\boldsymbol{\Omega}$.

Exercise 4.4: Provide a counterexample in which it is not possible to center the loadings with a rotation. Hint: use a one-factor model.

Exercise 4.5: Find conditions under which matrix (4.4) is invertible.
        - ValueError: If the model has not been fitted yet.
        """
        if 'factor_variances' not in self.intermediate:
            raise ValueError("The model has not been fitted yet. Call the 'fit' method first.")
        return self.intermediate['factor_variances']

    def get_lambda_bar(self):
        """
        Get lambda_bar, the average idiosyncratic variance.

        Returns:
        - lambda_bar (float): Average idiosyncratic variance.

        Raises:
        - ValueError: If the model has not been fitted yet.
        """
        if 'lambda_bar' not in self.intermediate:
            raise ValueError("The model has not been fitted yet. Call the 'fit' method first.")
        return self.intermediate['lambda_bar']

    def get_idiosyncratic_variance_matrix(self):
        """
        Get the idiosyncratic variance matrix D.

        Returns:
        - D (numpy.ndarray): Diagonal idiosyncratic variance matrix of shape (n_assets, n_assets).

        Raises:
        - ValueError: If the model has not been fitted yet.
        """
        if 'D' not in self.intermediate:
            raise ValueError("The model has not been fitted yet. Call the 'fit' method first.")
        return self.intermediate['D']

    def calculate_factor_returns(self, R_new):
        """
        Calculate factor returns given new return data.

        Parameters:
        - R_new (numpy.ndarray or pandas.DataFrame): New return matrix of shape (n_assets, n_time_periods).

        Returns:
        - f_new (numpy.ndarray): Factor returns of shape (n_second_pca_component, n_time_periods).

        Raises:
        - ValueError: If the model has not been fitted yet.
        """
        if isinstance(R_new, pd.DataFrame):
            R_new = R_new.values
        required_keys = ['U_second_pca', 'W_idio']
        for key in required_keys:
            if key not in self.intermediate:
                raise ValueError(f"The model has not been fitted yet. Missing '{key}'.")
        U_second_pca = self.intermediate['U_second_pca']
        W_idio = self.intermediate['W_idio']
        f_new = U_second_pca.T @ W_idio @ R_new
        
        return f_new

    def calculate_idio_returns(self, R_new):
        """
        Calculate asset idiosyncratic returns given new return data.

        Parameters:
        - R_new (numpy.ndarray or pandas.DataFrame): New return matrix of shape (n_assets, n_time_periods).

        Returns:
        - idio_new (numpy.ndarray): Idiosyncratic returns of shape (n_assets, n_time_periods).

        Raises:
        - ValueError: If the model has not been fitted yet.
        """
        if isinstance(R_new, pd.DataFrame):
            R_new = R_new.values
        required_keys = ['U_second_pca', 'W_idio', 'B']
        for key in required_keys:
            if key not in self.intermediate:
                raise ValueError(f"The model has not been fitted yet. Missing '{key}'.")
        f_new = self.calculate_factor_returns(R_new)
        B = self.intermediate['B']
        idio_new = R_new - B @ f_new
        
        return idio_new
