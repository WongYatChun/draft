import numpy as np
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.datasets import make_regression
from skopt import gp_minimize
from skopt.space import Real
from skopt.utils import use_named_args

# Assume PurgedKFold is defined externally and is imported
from external_module import PurgedKFold  # Replace with actual import

class BayesianLassoCV(BaseEstimator, RegressorMixin):
    def __init__(self, search_space=None, n_calls=50, cv=None, random_state=42):
        self.search_space = search_space
        self.n_calls = n_calls
        self.cv = cv
        self.random_state = random_state
        self.best_alpha_ = None
        self.model_ = None
        self.cv_results_ = []

    def fit(self, X, y):
        @use_named_args(self.search_space)
        def objective(**params):
            model = Lasso(**params)
            scores = cross_val_score(model, X, y, cv=self.cv, scoring='neg_mean_squared_error')
            self.cv_results_.append({
                'params': params,
                'mean_score': np.mean(scores),
                'std_score': np.std(scores)
            })
            return -np.mean(scores)

        result = gp_minimize(objective, self.search_space, n_calls=self.n_calls, random_state=self.random_state, verbose=1)

        self.best_alpha_ = result.x[0]
        self.model_ = Lasso(alpha=self.best_alpha_)
        self.model_.fit(X, y)
        return self

    def predict(self, X):
        return self.model_.predict(X)

    def score(self, X, y):
        return self.model_.score(X, y)

    def get_best_params(self):
        return {'alpha': self.best_alpha_}

    def get_cv_results(self):
        return self.cv_results_

# Generate synthetic data
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the search space for alpha
search_space = [
    Real(1e-3, 10, prior='log-uniform', name='alpha')
]

# Create a custom cross-validation instance
purged_kf = PurgedKFold(n_splits=5, purge_length=10, embargo=0.0)

# Instantiate the custom BayesianLassoCV model
bayesian_lasso_cv = BayesianLassoCV(search_space=search_space, n_calls=50, cv=purged_kf, random_state=42)

# Fit the model
bayesian_lasso_cv.fit(X_train, y_train)

# Evaluate on the validation set
val_score = bayesian_lasso_cv.score(X_val, y_val)
print(f"Validation R^2 Score: {val_score}")
print(f"Best Alpha: {bayesian_lasso_cv.best_alpha_}")

# Retrieve and print cross-validation results
cv_results = bayesian_lasso_cv.get_cv_results()
for i, result in enumerate(cv_results):
    print(f"Run {i + 1}: Params: {result['params']}, Mean Score: {result['mean_score']}, Std Score: {result['std_score']}")

# Optional: Plot convergence
import matplotlib.pyplot as plt

mean_scores = [-res['mean_score'] for res in cv_results]
plt.plot(mean_scores)
plt.xlabel('Iteration')
plt.ylabel('Negative MSE')
plt.title('Convergence Plot')
plt.show()
